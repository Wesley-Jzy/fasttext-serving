#!/usr/bin/env python3
"""
The-Stack-v2 å¤§è§„æ¨¡ä»£ç è´¨é‡åˆ†ç±»å®¢æˆ·ç«¯
"""

import asyncio
import aiohttp
import pandas as pd
import json
import time
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import signal
import sys
import os
from concurrent.futures import ThreadPoolExecutor

# å¯¼å…¥æ–‡ä»¶æ£€æµ‹å™¨
sys.path.append(str(Path(__file__).parent.parent))
from implementations.python.file_detector import IncrementalFileDetector

@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½®"""
    data_dir: str
    output_dir: str
    api_url: str
    max_concurrent: int = 50
    batch_size: int = 200
    timeout: int = 30
    stability_window: int = 30  # æ–‡ä»¶ç¨³å®šæ€§æ£€æµ‹çª—å£
    resume: bool = True  # æ–­ç‚¹ç»­ä¼ 
    save_format: str = "parquet"  # è¾“å‡ºæ ¼å¼: parquet æˆ– json
    log_level: str = "INFO"

@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    processed_files: int = 0
    skipped_files: int = 0
    total_samples: int = 0
    processed_samples: int = 0
    successful_samples: int = 0
    failed_samples: int = 0
    start_time: float = 0
    current_file: str = ""
    processing_time: float = 0
    throughput_sps: float = 0  # samples per second

class TheStackProcessor:
    """The-Stack-v2æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.stats = ProcessingStats()
        self.session: Optional[aiohttp.ClientSession] = None
        self.file_detector = IncrementalFileDetector(config.stability_window)
        self.shutdown_event = asyncio.Event()
        self.checkpoint_file = Path(config.output_dir) / "processing_checkpoint.json"
        self.processed_files = set()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if config.resume:
            self.load_checkpoint()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config.log_level.upper())
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # æ–‡ä»¶å¤„ç†å™¨
        log_file = Path(self.config.output_dir) / "processing.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        # é…ç½®logger
        self.logger = logging.getLogger('TheStackProcessor')
        self.logger.setLevel(log_level)
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
    
    def load_checkpoint(self):
        """åŠ è½½å¤„ç†æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint = json.load(f)
                    self.processed_files = set(checkpoint.get('processed_files', []))
                    self.logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: å·²å¤„ç† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def save_checkpoint(self):
        """ä¿å­˜å¤„ç†æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'processed_files': list(self.processed_files),
                'last_update': time.time(),
                'stats': asdict(self.stats)
            }
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # é…ç½®HTTPä¼šè¯
        connector = aiohttp.TCPConnector(
            limit=self.config.max_concurrent * 2,
            limit_per_host=self.config.max_concurrent,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        
        # å¥åº·æ£€æŸ¥
        await self.health_check()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def health_check(self):
        """APIå¥åº·æ£€æŸ¥"""
        try:
            async with self.session.get(f"{self.config.api_url}/health") as response:
                if response.status == 200:
                    health_info = await response.json()
                    self.logger.info(f"âœ… APIæœåŠ¡å¥åº·: {health_info.get('status', 'unknown')}")
                else:
                    self.logger.warning(f"âš ï¸ APIå¥åº·æ£€æŸ¥å¼‚å¸¸: HTTP {response.status}")
        except Exception as e:
            self.logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡: {e}")
            raise
    
    def preprocess_content(self, content: str) -> Optional[str]:
        """é¢„å¤„ç†ä»£ç å†…å®¹"""
        if not content or not isinstance(content, str):
            return None
        
        # åŸºæœ¬æ¸…ç†
        content = content.strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(content) == 0:
            return None
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šé¢„å¤„ç†é€»è¾‘
        return content
    
    async def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        try:
            async with self.session.post(
                f"{self.config.api_url}/predict",
                json=texts,
                params={"k": 2, "threshold": 0.0}
            ) as response:
                if response.status == 200:
                    results = await response.json()
                    
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    formatted_results = []
                    for result in results:
                        if isinstance(result, dict) and "labels" in result:
                            # æ–°æ ¼å¼: {"labels": [...], "scores": [...]}
                            formatted_results.append({
                                "labels": result["labels"],
                                "scores": result["scores"],
                                "prediction": result["labels"][0] if result["labels"] else "__label__0",
                                "confidence": result["scores"][0] if result["scores"] else 0.0
                            })
                        elif isinstance(result, (list, tuple)) and len(result) == 2:
                            # æ—§æ ¼å¼: [labels, scores]
                            labels, scores = result
                            formatted_results.append({
                                "labels": labels,
                                "scores": scores,
                                "prediction": labels[0] if labels else "__label__0",
                                "confidence": scores[0] if scores else 0.0
                            })
                        else:
                            # å¼‚å¸¸æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            formatted_results.append({
                                "labels": ["__label__0"],
                                "scores": [0.0],
                                "prediction": "__label__0",
                                "confidence": 0.0
                            })
                    
                    return formatted_results
                else:
                    error_text = await response.text()
                    self.logger.error(f"APIé”™è¯¯: HTTP {response.status} - {error_text}")
                    # è¿”å›é»˜è®¤ç»“æœ
                    return [self._get_default_prediction() for _ in texts]
                    
        except Exception as e:
            self.logger.error(f"é¢„æµ‹è¯·æ±‚å¼‚å¸¸: {e}")
            return [self._get_default_prediction() for _ in texts]
    
    def _get_default_prediction(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é¢„æµ‹ç»“æœ"""
        return {
            "labels": ["__label__0"],
            "scores": [0.0],
            "prediction": "__label__0",
            "confidence": 0.0
        }
    
    def postprocess_results(self, original_data: List[Dict], predictions: List[Dict]) -> List[Dict]:
        """åå¤„ç†ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
        results = []
        
        for original, prediction in zip(original_data, predictions):
            # éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§
            is_valid, error_reason = self.validate_prediction(prediction)
            
            # æ„å»ºè¾“å‡ºè®°å½•
            result = {
                # ä¿ç•™å…³é”®åŸå§‹å­—æ®µ
                "blob_id": original.get("blob_id"),
                "path": original.get("path"),
                "repo_name": original.get("repo_name"),
                "language": original.get("language"),
                "size": original.get("size"),
                "ext": original.get("ext"),
                
                # æ·»åŠ è´¨é‡åˆ†ç±»ç»“æœ
                "quality_labels": prediction["labels"],
                "quality_scores": prediction["scores"],
                "quality_prediction": prediction["prediction"],
                "quality_confidence": prediction["confidence"],
                
                # æ•°æ®æœ‰æ•ˆæ€§æ ‡è®°
                "prediction_valid": is_valid,
                "prediction_error": error_reason if not is_valid else None,
                
                # å¤„ç†å…ƒä¿¡æ¯
                "content_length": len(str(original.get("content", ""))),
                "processed_at": pd.Timestamp.now().isoformat()
            }
            
            results.append(result)
        
        return results
    
    def validate_prediction(self, prediction: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        """éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§"""
        try:
            labels = prediction.get("labels", [])
            scores = prediction.get("scores", [])
            
            # åŸºæœ¬æ ¼å¼æ£€æŸ¥
            if not isinstance(labels, list) or not isinstance(scores, list):
                return False, "invalid_format_not_list"
            
            if len(labels) != len(scores):
                return False, "length_mismatch"
            
            if len(labels) == 0:
                return False, "empty_prediction"
            
            # æ£€æŸ¥æ ‡ç­¾æ ¼å¼
            valid_labels = {"__label__0", "__label__1"}
            for label in labels:
                if label not in valid_labels:
                    return False, f"invalid_label_{label}"
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„æ ‡ç­¾ç»„åˆ
            label_set = set(labels)
            if len(label_set) > 2:
                return False, f"too_many_unique_labels_{len(label_set)}"
            
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦æŒ‰é™åºæ’åˆ—
            if scores != sorted(scores, reverse=True):
                return False, "scores_not_descending"
            
            return True, None
            
        except Exception as e:
            return False, f"validation_exception_{str(e)}"
    
    async def process_file(self, file_path: Path, semaphore: asyncio.Semaphore) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        async with semaphore:
            start_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if self.config.resume and str(file_path) in self.processed_files:
                self.logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                self.stats.skipped_files += 1
                return True
            
            self.stats.current_file = file_path.name
            self.logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†: {file_path.name}")
            
            try:
                # åŠ è½½æ•°æ®
                df = pd.read_parquet(file_path)
                
                if len(df) == 0:
                    self.logger.warning(f"âš ï¸ ç©ºæ–‡ä»¶: {file_path.name}")
                    self.processed_files.add(str(file_path))
                    return True
                
                # æ£€æŸ¥å¿…éœ€åˆ—
                if 'content' not in df.columns:
                    self.logger.error(f"âŒ ç¼ºå°‘contentåˆ—: {file_path.name}")
                    return False
                
                # é¢„å¤„ç†æ•°æ®
                valid_data = []
                valid_texts = []
                
                for idx, row in df.iterrows():
                    content = self.preprocess_content(row.get('content', ''))
                    if content:
                        valid_data.append(row.to_dict())
                        valid_texts.append(content)
                
                if not valid_texts:
                    self.logger.warning(f"âš ï¸ æ— æœ‰æ•ˆå†…å®¹: {file_path.name}")
                    self.processed_files.add(str(file_path))
                    return True
                
                self.logger.info(f"ğŸ“Š {file_path.name}: å¤„ç† {len(valid_texts)} ä¸ªæ ·æœ¬")
                self.stats.total_samples += len(valid_texts)
                
                # åˆ†æ‰¹å¤„ç†
                all_results = []
                batch_count = 0
                
                for i in range(0, len(valid_texts), self.config.batch_size):
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å…³é—­
                    if self.shutdown_event.is_set():
                        self.logger.info("ğŸ›‘ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œåœæ­¢å¤„ç†")
                        return False
                    
                    batch_texts = valid_texts[i:i + self.config.batch_size]
                    batch_data = valid_data[i:i + self.config.batch_size]
                    
                    batch_count += 1
                    self.logger.debug(f"  æ‰¹æ¬¡ {batch_count}: {len(batch_texts)} æ ·æœ¬")
                    
                    # APIé¢„æµ‹
                    predictions = await self.predict_batch(batch_texts)
                    
                    # åå¤„ç†
                    batch_results = self.postprocess_results(batch_data, predictions)
                    all_results.extend(batch_results)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats.processed_samples += len(batch_texts)
                    
                    # ç»Ÿè®¡æœ‰æ•ˆé¢„æµ‹å’Œæ— æ•ˆé¢„æµ‹
                    valid_count = 0
                    for result in batch_results:
                        if result.get("prediction_valid", True):
                            valid_count += 1
                        else:
                            # è®°å½•æ— æ•ˆé¢„æµ‹
                            self.logger.warning(f"æ— æ•ˆé¢„æµ‹: {result.get('prediction_error', 'unknown_error')}")
                    
                    self.stats.successful_samples += valid_count
                    self.stats.failed_samples += len(batch_texts) - valid_count
                
                # ä¿å­˜ç»“æœ
                success = await self.save_file_results(file_path, all_results)
                
                if success:
                    # æ›´æ–°ç»Ÿè®¡å’Œæ£€æŸ¥ç‚¹
                    self.processed_files.add(str(file_path))
                    self.stats.processed_files += 1
                    
                    processing_time = time.time() - start_time
                    self.stats.processing_time += processing_time
                    
                    throughput = len(valid_texts) / processing_time
                    self.logger.info(f"âœ… å®Œæˆ: {file_path.name} "
                                   f"({len(valid_texts)} æ ·æœ¬, {throughput:.1f} samples/sec)")
                    
                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if self.stats.processed_files % 5 == 0:
                        self.save_checkpoint()
                    
                    return True
                else:
                    return False
                    
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path.name} - {e}")
                return False
    
    async def save_file_results(self, original_file: Path, results: List[Dict]) -> bool:
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœ"""
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_name = f"processed_{original_file.stem}.{self.config.save_format}"
            output_path = Path(self.config.output_dir) / output_name
            
            if self.config.save_format == "parquet":
                df = pd.DataFrame(results)
                df.to_parquet(output_path, index=False)
            else:  # json
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.debug(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def update_progress_display(self):
        """æ›´æ–°è¿›åº¦æ˜¾ç¤º"""
        if self.stats.total_files > 0:
            file_progress = (self.stats.processed_files / self.stats.total_files) * 100
        else:
            file_progress = 0
        
        if self.stats.processing_time > 0:
            current_throughput = self.stats.processed_samples / self.stats.processing_time
            self.stats.throughput_sps = current_throughput
        else:
            current_throughput = 0
        
        elapsed_time = time.time() - self.stats.start_time
        
        print(f"\rğŸ“Š è¿›åº¦: {self.stats.processed_files}/{self.stats.total_files} æ–‡ä»¶ "
              f"({file_progress:.1f}%) | "
              f"{self.stats.processed_samples} æ ·æœ¬ | "
              f"{current_throughput:.1f} samples/sec | "
              f"ç”¨æ—¶: {elapsed_time:.0f}s", end="", flush=True)
    
    async def run(self):
        """è¿è¡Œä¸»å¤„ç†æµç¨‹"""
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†The-Stack-v2æ•°æ®")
        self.logger.info(f"æ•°æ®ç›®å½•: {self.config.data_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.config.output_dir}")
        self.logger.info(f"APIåœ°å€: {self.config.api_url}")
        self.logger.info(f"å¹¶å‘æ•°: {self.config.max_concurrent}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        
        data_dir = Path(self.config.data_dir)
        if not data_dir.exists():
            self.logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return
        
        self.stats.start_time = time.time()
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        def signal_handler(signum, frame):
            self.logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡ä¼˜é›…å…³é—­...")
            self.shutdown_event.set()
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        try:
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(self.config.max_concurrent)
            
            while not self.shutdown_event.is_set():
                # æ‰«æå¯å¤„ç†çš„æ–‡ä»¶
                ready_files = self.file_detector.scan_ready_files(data_dir)
                
                # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
                if self.config.resume:
                    new_files = [f for f in ready_files if str(f) not in self.processed_files]
                else:
                    new_files = ready_files
                
                if not new_files:
                    self.logger.info("â³ æš‚æ— æ–°æ–‡ä»¶ï¼Œç­‰å¾…30ç§’åé‡æ–°æ‰«æ...")
                    await asyncio.sleep(30)
                    continue
                
                self.stats.total_files += len(new_files)
                self.logger.info(f"ğŸ“ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶å¾…å¤„ç†")
                
                # é€ä¸ªå¤„ç†æ–‡ä»¶ï¼ˆæ–‡ä»¶çº§ä¸²è¡Œï¼‰
                for file_path in new_files:
                    if self.shutdown_event.is_set():
                        break
                    
                    # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                    self.update_progress_display()
                    
                    # å¤„ç†æ–‡ä»¶
                    success = await self.process_file(file_path, semaphore)
                    
                    if not success:
                        self.logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè·³è¿‡: {file_path.name}")
                
                # æ¸…ç†æ–‡ä»¶çŠ¶æ€ç¼“å­˜
                self.file_detector.cleanup_states(data_dir)
                
                # å¦‚æœæ²¡æœ‰æ›´å¤šæ–‡ä»¶ï¼Œç­‰å¾…æ–°æ–‡ä»¶
                if not self.shutdown_event.is_set():
                    self.logger.info("âœ… å½“å‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾…æ–°æ–‡ä»¶...")
                    await asyncio.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿå†æ¬¡æ‰«æ
        
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        
        finally:
            # æœ€ç»ˆä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint()
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - self.stats.start_time
            print(f"\n\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
            print(f"  æ€»å¤„ç†æ—¶é—´: {total_time:.1f} ç§’")
            print(f"  å¤„ç†æ–‡ä»¶æ•°: {self.stats.processed_files}")
            print(f"  è·³è¿‡æ–‡ä»¶æ•°: {self.stats.skipped_files}")
            print(f"  å¤„ç†æ ·æœ¬æ•°: {self.stats.processed_samples}")
            print(f"  æˆåŠŸæ ·æœ¬æ•°: {self.stats.successful_samples}")
            print(f"  å¤±è´¥æ ·æœ¬æ•°: {self.stats.failed_samples}")
            if self.stats.processed_samples > 0:
                success_rate = (self.stats.successful_samples / self.stats.processed_samples) * 100
                print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
            if total_time > 0:
                overall_throughput = self.stats.processed_samples / total_time
                print(f"  å¹³å‡ååé‡: {overall_throughput:.1f} samples/sec")

async def main():
    parser = argparse.ArgumentParser(
        description='The-Stack-v2 å¤§è§„æ¨¡ä»£ç è´¨é‡åˆ†ç±»å¤„ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python3 the_stack_processor.py \\
    --data-dir /path/to/the-stack-v2 \\
    --output-dir /path/to/results \\
    --api-url http://fasttext-api:8000

  # é«˜æ€§èƒ½é…ç½®
  python3 the_stack_processor.py \\
    --data-dir /path/to/the-stack-v2 \\
    --output-dir /path/to/results \\
    --api-url http://fasttext-api:8000 \\
    --max-concurrent 80 \\
    --batch-size 200

  # ä»æ–­ç‚¹ç»§ç»­å¤„ç†
  python3 the_stack_processor.py \\
    --data-dir /path/to/the-stack-v2 \\
    --output-dir /path/to/results \\
    --api-url http://fasttext-api:8000 \\
    --resume
        """
    )
    
    parser.add_argument('--data-dir', required=True,
                        help='The-Stack-v2æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', required=True,
                        help='å¤„ç†ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--api-url', required=True,
                        help='FastText APIæœåŠ¡åœ°å€')
    parser.add_argument('--max-concurrent', type=int, default=50,
                        help='æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 50)')
    parser.add_argument('--batch-size', type=int, default=200,
                        help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 200)')
    parser.add_argument('--timeout', type=int, default=30,
                        help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 30)')
    parser.add_argument('--stability-window', type=int, default=30,
                        help='æ–‡ä»¶ç¨³å®šæ€§æ£€æµ‹çª—å£(ç§’) (é»˜è®¤: 30)')
    parser.add_argument('--resume', action='store_true',
                        help='ä»æ–­ç‚¹ç»§ç»­å¤„ç†')
    parser.add_argument('--save-format', choices=['parquet', 'json'], default='parquet',
                        help='è¾“å‡ºæ–‡ä»¶æ ¼å¼ (é»˜è®¤: parquet)')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = ProcessingConfig(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        api_url=args.api_url,
        max_concurrent=args.max_concurrent,
        batch_size=args.batch_size,
        timeout=args.timeout,
        stability_window=args.stability_window,
        resume=args.resume,
        save_format=args.save_format,
        log_level=args.log_level
    )
    
    # è¿è¡Œå¤„ç†å™¨
    async with TheStackProcessor(config) as processor:
        await processor.run()

if __name__ == "__main__":
    asyncio.run(main())
