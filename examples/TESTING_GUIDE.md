# FastText Serving æµ‹è¯•æŒ‡å—

æœ¬æŒ‡å—å°†å¸®ä½ å®Œæˆä»æ„å»ºåˆ°æµ‹è¯•çš„å®Œæ•´æµç¨‹ï¼ŒéªŒè¯cookingé—®é¢˜åˆ†ç±»åŠŸèƒ½ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ¨¡å‹

ä½¿ç”¨ç°æœ‰çš„cookingæ¨¡å‹ï¼š

```bash
# æ£€æŸ¥ç°æœ‰æ¨¡å‹
ls -la models/
file models/cooking.model.bin

# ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨å’Œå¯è¯»
chmod 644 models/cooking.model.bin
```

### 2. æ„å»ºå’Œè¿è¡Œ FastText Serving

#### æ–¹æ³• A: Docker æ–¹å¼ï¼ˆæ¨èï¼‰

```bash
# æ„å»º Docker é•œåƒ
docker build -f Dockerfile.test -t fasttext-serving:test .

# è¿è¡ŒæœåŠ¡ï¼ˆæŒ‚è½½æ¨¡å‹ï¼‰
docker run -d \
  --name fasttext-serving \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  fasttext-serving:test \
  --model /app/models/cooking.model.bin \
  --address 0.0.0.0 \
  --port 8000 \
  --max-request-size 500

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker logs fasttext-serving
curl http://localhost:8000/health
```

#### æ–¹æ³• B: æœ¬åœ°ç¼–è¯‘æ–¹å¼

```bash
# ç¼–è¯‘ï¼ˆéœ€è¦ Rust ç¯å¢ƒï¼‰
cargo build --release

# è¿è¡ŒæœåŠ¡
./target/release/fasttext-serving \
  --model models/cooking.model.bin \
  --address 127.0.0.1 \
  --port 8000 \
  --max-request-size 500 \
  --workers 4
```

### 3. æµ‹è¯•å®¢æˆ·ç«¯

```bash
cd examples

# ä½¿ç”¨æµ‹è¯•æ•°æ®å¿«é€ŸéªŒè¯
python client.py --use-test-data --output cooking_test_results.json

# æŸ¥çœ‹ç»“æœ
cat cooking_test_results.json | jq '.[] | {id, text, category_prediction, category_confidence}' | head -10
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### æ‰¹å¤„ç†èƒ½åŠ›æµ‹è¯•

```bash
# æµ‹è¯•å¤§æ‰¹é‡å¤„ç†ï¼ˆ600ä¸ªæ ·æœ¬ï¼‰
python client.py \
  --use-test-data \
  --batch-size 5000 \
  --max-concurrent 8 \
  --output large_batch_cooking_results.parquet

# ç›‘æ§å¤„ç†æ€§èƒ½
time python client.py \
  --use-test-data \
  --batch-size 2000 \
  --max-concurrent 4 \
  --output performance_cooking_test.json
```

### å¹¶å‘å‹åŠ›æµ‹è¯•

```bash
# å¤šè¿›ç¨‹å¹¶å‘æµ‹è¯•
for i in {1..5}; do
  python client.py \
    --use-test-data \
    --batch-size 1000 \
    --max-concurrent 2 \
    --output "concurrent_cooking_test_$i.json" &
done
wait

# æ£€æŸ¥æ‰€æœ‰ç»“æœ
ls -la concurrent_cooking_test_*.json
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### æœåŠ¡ç«¯é…ç½®

æ ¹æ®ä½ çš„ç¡¬ä»¶èµ„æºè°ƒæ•´å‚æ•°ï¼š

```bash
# é«˜æ€§èƒ½é…ç½®ï¼ˆå¤šæ ¸æœåŠ¡å™¨ï¼‰
fasttext-serving \
  --model models/cooking.model.bin \
  --workers 16 \
  --max-request-size 1000 \
  --address 0.0.0.0 \
  --port 8000

# å†…å­˜å—é™é…ç½®
fasttext-serving \
  --model models/cooking.model.bin \
  --workers 4 \
  --max-request-size 200 \
  --address 127.0.0.1 \
  --port 8000
```

### å®¢æˆ·ç«¯é…ç½®

```bash
# é«˜ååé‡é…ç½®
python client.py \
  --batch-size 10000 \
  --max-concurrent 16 \
  --use-test-data

# ç¨³å®šæ€§ä¼˜å…ˆé…ç½®  
python client.py \
  --batch-size 1000 \
  --max-concurrent 4 \
  --use-test-data
```

## ğŸ› é—®é¢˜æ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la models/
file models/cooking.model.bin

# æ£€æŸ¥æ–‡ä»¶æƒé™
chmod 644 models/cooking.model.bin
```

#### 2. è¯·æ±‚è¶…æ—¶

```bash
# å¢åŠ å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´
# ä¿®æ”¹ client.py ä¸­çš„ timeout å‚æ•°
timeout = aiohttp.ClientTimeout(total=600)  # 10åˆ†é’Ÿ
```

#### 3. å†…å­˜ä¸è¶³

```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
docker stats fasttext-serving

# å‡å°‘æ‰¹å¤„ç†å¤§å°
python client.py --batch-size 500 --max-concurrent 2
```

#### 4. JSON è§£æé”™è¯¯

```bash
# æ£€æŸ¥è¯·æ±‚å¤§å°
python -c "
import json
data = ['test'] * 100000
print(f'Size: {len(json.dumps(data)) / 1024 / 1024:.1f} MB')
"

# è°ƒæ•´æœåŠ¡ç«¯é™åˆ¶
fasttext-serving --max-request-size 1000  # 1GB
```

### æ—¥å¿—è°ƒè¯•

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export RUST_LOG=fasttext_serving=debug
fasttext-serving --model models/cooking.model.bin

# Docker æ—¥å¿—
docker logs -f fasttext-serving

# å®¢æˆ·ç«¯è¯¦ç»†è¾“å‡º
python client.py --use-test-data -v
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡ï¼ˆcookingæ¨¡å‹ï¼‰

| é…ç½® | æ‰¹å¤„ç†å¤§å° | å¹¶å‘æ•° | é¢„æœŸååé‡ | å†…å­˜ä½¿ç”¨ |
|------|-----------|--------|------------|----------|
| å•æ ¸ | 1000 | 2 | 500 texts/sec | < 1GB |
| 4æ ¸ | 5000 | 8 | 2000 texts/sec | < 2GB |
| 8æ ¸ | 10000 | 16 | 5000 texts/sec | < 4GB |

### ä¼˜åŒ–å»ºè®®

1. **æ‰¹å¤„ç†å¤§å°**: 
   - çŸ­æ–‡æœ¬é—®é¢˜: 5000-10000
   - é•¿æ–‡æœ¬é—®é¢˜: 1000-2000

2. **å¹¶å‘é…ç½®**:
   - å®¢æˆ·ç«¯å¹¶å‘ = CPUæ ¸æ•° Ã— 2
   - æœåŠ¡ç«¯å·¥ä½œçº¿ç¨‹ = CPUæ ¸æ•°

3. **å†…å­˜ç®¡ç†**:
   - å•ä¸ªæ–‡æœ¬é™åˆ¶: 1MB
   - è¯·æ±‚å¤§å°é™åˆ¶: 500MB-1GB
   - é¢„ç•™ç³»ç»Ÿå†…å­˜: 20%

## ğŸ”„ è‡ªåŠ¨åŒ–æµ‹è¯•

åˆ›å»ºè‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼š

```bash
#!/bin/bash
# test_cooking_automation.sh

echo "ğŸš€ Starting FastText Serving cooking test..."

# 1. æ„å»ºå’Œå¯åŠ¨æœåŠ¡
docker build -f Dockerfile.test -t fasttext-serving:test .
docker run -d --name test-serving -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  fasttext-serving:test \
  --model /app/models/cooking.model.bin

# 2. ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# 3. è¿è¡Œæµ‹è¯•
cd examples
python client.py --use-test-data --output auto_cooking_test_results.json

# 4. éªŒè¯ç»“æœ
if [ -f "auto_cooking_test_results.json" ]; then
  echo "âœ… Test completed successfully"
  python -c "
import json
with open('auto_cooking_test_results.json') as f:
    data = json.load(f)
    print(f'Processed {len(data)} samples')
    predictions = [item['category_prediction'] for item in data]
    print(f'Categories found: {set(predictions)}')
"
else
  echo "âŒ Test failed"
  exit 1
fi

# 5. æ¸…ç†
docker stop test-serving
docker rm test-serving

echo "ğŸ‰ Cooking automation test completed!"
```

ä½¿ç”¨æ–¹æ³•ï¼š

```bash
chmod +x test_cooking_automation.sh
./test_cooking_automation.sh
```

## ğŸš¢ ç”Ÿäº§éƒ¨ç½²å‡†å¤‡

### K8s éƒ¨ç½²é…ç½®

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fasttext-serving
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fasttext-serving
  template:
    metadata:
      labels:
        app: fasttext-serving
    spec:
      containers:
      - name: fasttext-serving
        image: hub.anuttacon.com/infra/fasttext-serving:latest
        ports:
        - containerPort: 8000
        args:
          - "--model"
          - "/app/models/cooking.model.bin"
          - "--address"
          - "0.0.0.0"
          - "--workers"
          - "4"
          - "--max-request-size"
          - "500"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: model-volume
          mountPath: /app/models
      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: fasttext-model-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: fasttext-serving-service
spec:
  selector:
    app: fasttext-serving
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### ç¤ºä¾‹æµ‹è¯•æ•°æ®

cookingæ¨¡å‹å¯ä»¥åˆ†ç±»çš„é—®é¢˜ç±»å‹åŒ…æ‹¬ï¼š
- **baking**: çƒ˜ç„™ç›¸å…³é—®é¢˜
- **equipment**: å¨å…·è®¾å¤‡é—®é¢˜  
- **food-safety**: é£Ÿå“å®‰å…¨é—®é¢˜
- **preparation**: é£Ÿæå‡†å¤‡é—®é¢˜
- **cooking-method**: çƒ¹é¥ªæ–¹æ³•é—®é¢˜

è¿™ä¸ªæµ‹è¯•æŒ‡å—ä¸“é—¨é’ˆå¯¹cookingæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç°åœ¨ä½ å¯ä»¥æŒ‰ç…§æŒ‡å—é€æ­¥éªŒè¯ FastText serving çš„åŠŸèƒ½ã€‚ 