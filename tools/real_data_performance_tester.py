#!/usr/bin/env python3
"""
FastText APIçœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•å·¥å…·
ä½¿ç”¨çœŸå®The-Stack-v2æ•°æ®æµ‹è¯•ä¸åŒå¹¶å‘æ•°å’Œæ‰¹æ¬¡å¤§å°ä¸‹çš„APIæ€§èƒ½è¡¨ç°
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
import pandas as pd
import os
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    api_url: str
    data_dir: str
    concurrency_levels: List[int] = None
    batch_sizes: List[int] = None

@dataclass
class FileTestResult:
    """å•æ–‡ä»¶æµ‹è¯•ç»“æœ"""
    file_name: str
    file_size_mb: float
    file_size_gb: float
    total_samples: int
    processed_samples: int
    successful_samples: int
    failed_samples: int
    concurrency: int
    batch_size: int
    processing_time: float
    throughput_sps: float  # samples per second
    throughput_gbs: float  # GB per second (åŸå§‹æ•°æ®)
    avg_latency: float
    success_rate: float
    error_types: Dict[str, int]
    sample_errors: List[str]  # å‰å‡ ä¸ªé”™è¯¯æ ·æœ¬

class RealDataPerformanceTester:
    """çœŸå®æ•°æ®APIæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.session: aiohttp.ClientSession = None
        
    async def __aenter__(self):
        # é…ç½®è¿æ¥æ± 
        connector = aiohttp.TCPConnector(
            limit=1000,
            limit_per_host=500,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=60)  # å¢åŠ è¶…æ—¶æ—¶é—´
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def find_test_files(self) -> Tuple[Path, Path]:
        """æ‰¾åˆ°æœ€å¤§å’Œæœ€å°çš„parquetæ–‡ä»¶"""
        data_dir = Path(self.config.data_dir)
        if not data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
        parquet_files = list(data_dir.glob("*.parquet"))
        if len(parquet_files) < 2:
            raise ValueError(f"æ•°æ®ç›®å½•ä¸­parquetæ–‡ä»¶æ•°é‡ä¸è¶³: {len(parquet_files)}")
        
        # æŒ‰æ–‡ä»¶å¤§å°æ’åº
        files_with_size = [(f, f.stat().st_size) for f in parquet_files]
        files_with_size.sort(key=lambda x: x[1])
        
        smallest_file = files_with_size[0][0]
        largest_file = files_with_size[-1][0]
        
        print(f"ğŸ“ é€‰ä¸­æµ‹è¯•æ–‡ä»¶:")
        print(f"  æœ€å°æ–‡ä»¶: {smallest_file.name} ({files_with_size[0][1] / 1024**2:.1f} MB)")
        print(f"  æœ€å¤§æ–‡ä»¶: {largest_file.name} ({files_with_size[-1][1] / 1024**2:.1f} MB)")
        
        return smallest_file, largest_file
    
    def load_file_data(self, file_path: Path) -> Tuple[List[str], int, float]:
        """åŠ è½½æ–‡ä»¶æ•°æ®å¹¶é¢„å¤„ç†"""
        print(f"ğŸ“– åŠ è½½æ–‡ä»¶: {file_path.name}")
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size_bytes = file_path.stat().st_size
        file_size_mb = file_size_bytes / (1024 ** 2)
        file_size_gb = file_size_bytes / (1024 ** 3)
        
        try:
            # è¯»å–parquetæ–‡ä»¶
            df = pd.read_parquet(file_path)
            
            if 'content' not in df.columns:
                raise ValueError(f"æ–‡ä»¶ç¼ºå°‘contentåˆ—: {file_path.name}")
            
            # é¢„å¤„ç†å†…å®¹
            valid_texts = []
            for content in df['content']:
                if content and isinstance(content, str) and len(content.strip()) > 0:
                    valid_texts.append(content.strip())
            
            print(f"  æ€»æ ·æœ¬æ•°: {len(df):,}")
            print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_texts):,}")
            print(f"  æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB ({file_size_gb:.3f} GB)")
            
            return valid_texts, len(df), file_size_gb
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path.name} - {e}")
    
    async def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        try:
            async with self.session.post(
                f"{self.config.api_url}/predict",
                json=texts,
                params={"k": 2, "threshold": 0.0}
            ) as response:
                if response.status == 200:
                    results = await response.json()
                    
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    formatted_results = []
                    for result in results:
                        if isinstance(result, dict) and "labels" in result:
                            # æ–°æ ¼å¼: {"labels": [...], "scores": [...]}
                            formatted_results.append({
                                "labels": result["labels"],
                                "scores": result["scores"],
                                "prediction": result["labels"][0] if result["labels"] else "__label__0",
                                "confidence": result["scores"][0] if result["scores"] else 0.0
                            })
                        elif isinstance(result, (list, tuple)) and len(result) == 2:
                            # æ—§æ ¼å¼: [labels, scores]
                            labels, scores = result
                            formatted_results.append({
                                "labels": labels,
                                "scores": scores,
                                "prediction": labels[0] if labels else "__label__0",
                                "confidence": scores[0] if scores else 0.0
                            })
                        else:
                            # å¼‚å¸¸æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            formatted_results.append({
                                "labels": ["__label__0"],
                                "scores": [0.0],
                                "prediction": "__label__0",
                                "confidence": 0.0
                            })
                    
                    return formatted_results
                else:
                    error_text = await response.text()
                    raise RuntimeError(f"APIé”™è¯¯: HTTP {response.status} - {error_text}")
                    
        except Exception as e:
            # è¿”å›é»˜è®¤ç»“æœï¼Œä½†è®°å½•é”™è¯¯
            return [{
                "labels": ["__label__0"],
                "scores": [0.0],
                "prediction": "__label__0",
                "confidence": 0.0,
                "error": str(e)
            } for _ in texts]
    
    def validate_prediction(self, prediction: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        """éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§"""
        try:
            if "error" in prediction:
                return False, f"api_error_{prediction['error'][:50]}"
            
            labels = prediction.get("labels", [])
            scores = prediction.get("scores", [])
            
            # åŸºæœ¬æ ¼å¼æ£€æŸ¥
            if not isinstance(labels, list) or not isinstance(scores, list):
                return False, "invalid_format_not_list"
            
            if len(labels) != len(scores):
                return False, "length_mismatch"
            
            if len(labels) == 0:
                return False, "empty_prediction"
            
            # æ£€æŸ¥æ ‡ç­¾æ ¼å¼
            valid_labels = {"__label__0", "__label__1"}
            for label in labels:
                if label not in valid_labels:
                    return False, f"invalid_label_{label}"
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„æ ‡ç­¾ç»„åˆ
            label_set = set(labels)
            if len(label_set) > 2:
                return False, f"too_many_unique_labels_{len(label_set)}"
            
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦æŒ‰é™åºæ’åˆ—
            if scores != sorted(scores, reverse=True):
                return False, "scores_not_descending"
            
            return True, None
            
        except Exception as e:
            return False, f"validation_exception_{str(e)}"
    
    async def test_file_with_config(self, file_path: Path, concurrency: int, batch_size: int) -> FileTestResult:
        """ä½¿ç”¨æŒ‡å®šé…ç½®æµ‹è¯•å•ä¸ªæ–‡ä»¶"""
        print(f"\nğŸ§ª æµ‹è¯•æ–‡ä»¶: {file_path.name}")
        print(f"   é…ç½®: å¹¶å‘={concurrency}, æ‰¹æ¬¡å¤§å°={batch_size}")
        
        # åŠ è½½æ•°æ®
        texts, total_samples, file_size_gb = self.load_file_data(file_path)
        file_size_mb = file_size_gb * 1024
        
        if not texts:
            raise ValueError(f"æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹: {file_path.name}")
        
        # å¤„ç†ç»Ÿè®¡
        start_time = time.time()
        processed_samples = 0
        successful_samples = 0
        failed_samples = 0
        error_types = {}
        sample_errors = []
        latencies = []
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)
        
        async def process_batch(batch_texts: List[str]) -> Tuple[int, int, List[str], List[float]]:
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
            async with semaphore:
                batch_start = time.time()
                predictions = await self.predict_batch(batch_texts)
                batch_latency = time.time() - batch_start
                
                batch_successful = 0
                batch_failed = 0
                batch_errors = []
                
                for prediction in predictions:
                    is_valid, error_reason = self.validate_prediction(prediction)
                    if is_valid:
                        batch_successful += 1
                    else:
                        batch_failed += 1
                        if error_reason:
                            error_types[error_reason] = error_types.get(error_reason, 0) + 1
                            if len(batch_errors) < 5:  # åªä¿ç•™å‰å‡ ä¸ªé”™è¯¯æ ·æœ¬
                                batch_errors.append(error_reason)
                
                return batch_successful, batch_failed, batch_errors, [batch_latency]
        
        # åˆ†æ‰¹å¤„ç†
        tasks = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            tasks.append(process_batch(batch_texts))
        
        print(f"   å¼€å§‹å¤„ç† {len(tasks)} ä¸ªæ‰¹æ¬¡...")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        processing_time = time.time() - start_time
        
        # æ±‡æ€»ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                print(f"   âŒ æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {result}")
                continue
            
            batch_successful, batch_failed, batch_errors, batch_latencies = result
            successful_samples += batch_successful
            failed_samples += batch_failed
            sample_errors.extend(batch_errors)
            latencies.extend(batch_latencies)
        
        processed_samples = successful_samples + failed_samples
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        throughput_sps = processed_samples / processing_time if processing_time > 0 else 0
        throughput_gbs = file_size_gb / processing_time if processing_time > 0 else 0
        avg_latency = statistics.mean(latencies) if latencies else 0
        success_rate = successful_samples / processed_samples if processed_samples > 0 else 0
        
        print(f"   âœ… å®Œæˆ: {processed_samples}/{total_samples} æ ·æœ¬")
        print(f"   ğŸ“Š ååé‡: {throughput_sps:.1f} samples/sec, {throughput_gbs:.3f} GB/sec")
        print(f"   ğŸ¯ æˆåŠŸç‡: {success_rate:.1%}")
        
        return FileTestResult(
            file_name=file_path.name,
            file_size_mb=file_size_mb,
            file_size_gb=file_size_gb,
            total_samples=total_samples,
            processed_samples=processed_samples,
            successful_samples=successful_samples,
            failed_samples=failed_samples,
            concurrency=concurrency,
            batch_size=batch_size,
            processing_time=processing_time,
            throughput_sps=throughput_sps,
            throughput_gbs=throughput_gbs,
            avg_latency=avg_latency,
            success_rate=success_rate,
            error_types=error_types,
            sample_errors=sample_errors[:10]  # åªä¿ç•™å‰10ä¸ªé”™è¯¯
        )
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„çœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹çœŸå®æ•°æ®APIæ€§èƒ½æµ‹è¯•")
        print(f"ç›®æ ‡URL: {self.config.api_url}")
        print(f"æ•°æ®ç›®å½•: {self.config.data_dir}")
        
        # å¥åº·æ£€æŸ¥
        try:
            async with self.session.get(f"{self.config.api_url}/health") as response:
                if response.status == 200:
                    health_info = await response.json()
                    print(f"âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡: {health_info.get('status', 'unknown')}")
                else:
                    print(f"âš ï¸  æœåŠ¡å¥åº·æ£€æŸ¥å¼‚å¸¸: HTTP {response.status}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡: {e}")
            return {}
        
        # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
        try:
            smallest_file, largest_file = self.find_test_files()
        except Exception as e:
            print(f"âŒ é€‰æ‹©æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return {}
        
        # æµ‹è¯•ç»“æœ
        all_results = []
        
        # æµ‹è¯•ä¸¤ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ç”¨ä¸åŒé…ç½®
        test_files = [
            ("smallest", smallest_file),
            ("largest", largest_file)
        ]
        
        for file_type, file_path in test_files:
            print(f"\nğŸ“‚ å¼€å§‹æµ‹è¯• {file_type} æ–‡ä»¶: {file_path.name}")
            
            for concurrency in self.config.concurrency_levels:
                for batch_size in self.config.batch_sizes:
                    try:
                        result = await self.test_file_with_config(file_path, concurrency, batch_size)
                        result_dict = asdict(result)
                        result_dict['file_type'] = file_type
                        all_results.append(result_dict)
                        
                        # çŸ­æš‚ä¼‘æ¯é¿å…æœåŠ¡è¿‡è½½
                        await asyncio.sleep(1)
                        
                    except Exception as e:
                        print(f"âŒ æµ‹è¯•å¤±è´¥ ({file_type}, å¹¶å‘={concurrency}, æ‰¹æ¬¡={batch_size}): {e}")
        
        # åˆ†ææœ€ä½³é…ç½®
        best_config = self.analyze_results(all_results)
        
        return {
            "test_results": all_results,
            "best_configuration": best_config,
            "test_summary": self.generate_summary(all_results)
        }
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœï¼Œæ‰¾å‡ºæœ€ä¼˜é…ç½®"""
        if not results:
            return {}
        
        # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
        smallest_results = [r for r in results if r['file_type'] == 'smallest']
        largest_results = [r for r in results if r['file_type'] == 'largest']
        
        def find_best(file_results: List[Dict], criteria: str = "throughput_gbs") -> Dict:
            """æ‰¾åˆ°æœ€ä½³é…ç½®"""
            if not file_results:
                return {}
            
            # è¿‡æ»¤æˆåŠŸç‡é«˜çš„ç»“æœ
            valid_results = [r for r in file_results if r['success_rate'] >= 0.95]
            if not valid_results:
                valid_results = file_results
            
            # æŒ‰æŒ‡å®šæ ‡å‡†æ’åº
            best_result = max(valid_results, key=lambda x: x[criteria])
            return {
                "file_name": best_result["file_name"],
                "concurrency": best_result["concurrency"],
                "batch_size": best_result["batch_size"],
                "throughput_sps": best_result["throughput_sps"],
                "throughput_gbs": best_result["throughput_gbs"],
                "success_rate": best_result["success_rate"],
                "avg_latency": best_result["avg_latency"]
            }
        
        return {
            "smallest_file_best": find_best(smallest_results),
            "largest_file_best": find_best(largest_results),
            "overall_best_throughput": find_best(results, "throughput_gbs"),
            "overall_best_sps": find_best(results, "throughput_sps")
        }
    
    def generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        if not results:
            return {}
        
        total_processed = sum(r['processed_samples'] for r in results)
        total_successful = sum(r['successful_samples'] for r in results)
        total_time = sum(r['processing_time'] for r in results)
        total_data_gb = sum(r['file_size_gb'] for r in results)
        
        max_throughput_gbs = max(results, key=lambda x: x['throughput_gbs'])
        max_throughput_sps = max(results, key=lambda x: x['throughput_sps'])
        
        return {
            "total_tests": len(results),
            "total_samples_processed": total_processed,
            "total_successful_samples": total_successful,
            "overall_success_rate": total_successful / total_processed if total_processed > 0 else 0,
            "total_processing_time": total_time,
            "total_data_processed_gb": total_data_gb,
            "max_throughput_gbs": max_throughput_gbs['throughput_gbs'],
            "max_throughput_sps": max_throughput_sps['throughput_sps'],
            "best_config_for_gbs": {
                "concurrency": max_throughput_gbs['concurrency'],
                "batch_size": max_throughput_gbs['batch_size']
            },
            "best_config_for_sps": {
                "concurrency": max_throughput_sps['concurrency'],
                "batch_size": max_throughput_sps['batch_size']
            }
        }

def save_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def print_summary(results: Dict[str, Any]):
    """æ‰“å°æµ‹è¯•æ‘˜è¦"""
    if not results:
        return
    
    summary = results.get("test_summary", {})
    best_config = results.get("best_configuration", {})
    
    print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print(f"="*60)
    print(f"æ€»æµ‹è¯•æ•°: {summary.get('total_tests', 0)}")
    print(f"æ€»æ ·æœ¬æ•°: {summary.get('total_samples_processed', 0):,}")
    print(f"æˆåŠŸç‡: {summary.get('overall_success_rate', 0):.1%}")
    print(f"æ€»æ•°æ®é‡: {summary.get('total_data_processed_gb', 0):.3f} GB")
    print(f"æ€»å¤„ç†æ—¶é—´: {summary.get('total_processing_time', 0):.1f} ç§’")
    
    print(f"\nğŸ† æ€§èƒ½å³°å€¼:")
    print(f"æœ€é«˜æ•°æ®ååé‡: {summary.get('max_throughput_gbs', 0):.3f} GB/sec")
    print(f"æœ€é«˜æ ·æœ¬ååé‡: {summary.get('max_throughput_sps', 0):.1f} samples/sec")
    
    if "overall_best_throughput" in best_config:
        best = best_config["overall_best_throughput"]
        print(f"\nğŸ’¡ æ¨èé…ç½® (åŸºäºæ•°æ®ååé‡):")
        print(f"  å¹¶å‘æ•°: {best.get('concurrency', 0)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {best.get('batch_size', 0)}")
        print(f"  é¢„æœŸæ€§èƒ½: {best.get('throughput_gbs', 0):.3f} GB/sec")
        print(f"  é¢„æœŸæ ·æœ¬ç‡: {best.get('throughput_sps', 0):.1f} samples/sec")
        print(f"\nğŸ“‹ CLIå‘½ä»¤å»ºè®®:")
        print(f"  --max-concurrent {best.get('concurrency', 50)} --batch-size {best.get('batch_size', 200)}")

async def main():
    parser = argparse.ArgumentParser(
        description='FastText APIçœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--api-url', required=True, help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--data-dir', required=True, help='The-Stack-v2æ•°æ®ç›®å½•')
    parser.add_argument('--output', default='real_data_performance_results.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼(å°‘é‡é…ç½®)')
    
    args = parser.parse_args()
    
    # é…ç½®æµ‹è¯•å‚æ•°
    if args.quick:
        concurrency_levels = [20, 50]
        batch_sizes = [100, 200]
    else:
        concurrency_levels = [10, 20, 30, 50, 80, 100]
        batch_sizes = [50, 100, 200, 500]
    
    config = TestConfig(
        api_url=args.api_url,
        data_dir=args.data_dir,
        concurrency_levels=concurrency_levels,
        batch_sizes=batch_sizes
    )
    
    # è¿è¡Œæµ‹è¯•
    async with RealDataPerformanceTester(config) as tester:
        results = await tester.run_comprehensive_test()
        
        if results:
            # ä¿å­˜ç»“æœ
            save_results(results, args.output)
            
            # æ‰“å°æ‘˜è¦
            print_summary(results)
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼Œæ— ç»“æœç”Ÿæˆ")

if __name__ == "__main__":
    asyncio.run(main())
