#!/usr/bin/env python3
"""
FastText APIçœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•å·¥å…·
ä½¿ç”¨çœŸå®The-Stack-v2æ•°æ®æµ‹è¯•ä¸åŒå¹¶å‘æ•°å’Œæ‰¹æ¬¡å¤§å°ä¸‹çš„APIæ€§èƒ½è¡¨ç°
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
import pandas as pd
import os
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
from pathlib import Path

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    api_url: str
    data_dir: str
    concurrency_levels: List[int] = None
    batch_sizes: List[int] = None

@dataclass
class FileTestResult:
    """å•æ–‡ä»¶æµ‹è¯•ç»“æœ"""
    file_name: str
    file_size_mb: float
    file_size_gb: float
    total_samples: int
    processed_samples: int
    successful_samples: int
    failed_samples: int
    concurrency: int
    batch_size: int
    processing_time: float
    throughput_sps: float  # samples per second
    throughput_gbs: float  # GB per second (åŸå§‹æ•°æ®)
    avg_latency: float
    success_rate: float
    error_types: Dict[str, int]
    sample_errors: List[str]  # å‰å‡ ä¸ªé”™è¯¯æ ·æœ¬

class APIPerformanceTester:
    """APIæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.session: aiohttp.ClientSession = None
        
    async def __aenter__(self):
        # é…ç½®è¿æ¥æ± 
        connector = aiohttp.TCPConnector(
            limit=1000,  # æ€»è¿æ¥æ± å¤§å°
            limit_per_host=500,  # æ¯ä¸ªä¸»æœºè¿æ¥æ•°
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def generate_test_texts(self, count: int = 1000) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•æ–‡æœ¬"""
        # åŸºç¡€ä»£ç æ ·æœ¬
        base_samples = [
            "def hello_world():\n    print('Hello, World!')\n    return True",
            "import numpy as np\ndef calculate_mean(data):\n    return np.mean(data)",
            "class DataProcessor:\n    def __init__(self):\n        self.data = []\n    def process(self):\n        pass",
            "async def fetch_data(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.json()",
            "from typing import List, Dict\ndef merge_dicts(dicts: List[Dict]) -> Dict:\n    result = {}\n    for d in dicts:\n        result.update(d)\n    return result",
            "SELECT users.name, COUNT(orders.id) as order_count\nFROM users\nLEFT JOIN orders ON users.id = orders.user_id\nGROUP BY users.name",
            "function calculateTotal(items) {\n    return items.reduce((sum, item) => sum + item.price, 0);\n}",
            "package main\nimport \"fmt\"\nfunc main() {\n    fmt.Println(\"Hello, Go!\")\n}",
            "#include <stdio.h>\nint main() {\n    printf(\"Hello, C!\\n\");\n    return 0;\n}",
            "pub fn fibonacci(n: u32) -> u32 {\n    match n {\n        0 => 0,\n        1 => 1,\n        _ => fibonacci(n-1) + fibonacci(n-2)\n    }\n}"
        ]
        
        # ç”ŸæˆæŒ‡å®šæ•°é‡çš„æµ‹è¯•æ–‡æœ¬
        test_texts = []
        for i in range(count):
            base_idx = i % len(base_samples)
            # æ·»åŠ ä¸€äº›å˜åŒ–ä½¿æ–‡æœ¬ç•¥æœ‰ä¸åŒ
            text = f"// Sample {i+1}\n{base_samples[base_idx]}\n// End of sample"
            test_texts.append(text)
        
        return test_texts
    
    async def single_request(self, texts: List[str]) -> Tuple[bool, float, str]:
        """æ‰§è¡Œå•æ¬¡APIè¯·æ±‚"""
        start_time = time.time()
        
        try:
            async with self.session.post(
                f"{self.config.api_url}/predict",
                json=texts,
                params={"k": 2, "threshold": 0.0}
            ) as response:
                latency = time.time() - start_time
                
                if response.status == 200:
                    result = await response.json()
                    # éªŒè¯è¿”å›ç»“æœæ ¼å¼
                    if isinstance(result, list) and len(result) == len(texts):
                        return True, latency, "success"
                    else:
                        return False, latency, f"invalid_response_format"
                else:
                    error_text = await response.text()
                    return False, latency, f"http_{response.status}"
                    
        except asyncio.TimeoutError:
            latency = time.time() - start_time
            return False, latency, "timeout"
        except Exception as e:
            latency = time.time() - start_time
            return False, latency, f"exception_{type(e).__name__}"
    
    async def run_load_test(self, concurrency: int, batch_size: int) -> TestResult:
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: å¹¶å‘={concurrency}, æ‰¹æ¬¡å¤§å°={batch_size}")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_texts = self.generate_test_texts(1000)
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)
        
        # ç»Ÿè®¡æ•°æ®
        results = []
        error_types = {}
        start_test_time = time.time()
        
        async def worker():
            """å·¥ä½œåç¨‹"""
            while time.time() - start_test_time < self.config.test_duration:
                # éšæœºé€‰æ‹©æ‰¹æ¬¡æ–‡æœ¬
                batch_start = (len(results) * batch_size) % (len(test_texts) - batch_size)
                batch_texts = test_texts[batch_start:batch_start + batch_size]
                
                async with semaphore:
                    success, latency, error_type = await self.single_request(batch_texts)
                    
                    results.append({
                        'success': success,
                        'latency': latency,
                        'error_type': error_type,
                        'batch_size': batch_size
                    })
                    
                    if not success:
                        error_types[error_type] = error_types.get(error_type, 0) + 1
        
        # é¢„çƒ­é˜¶æ®µ
        print(f"  ğŸ”¥ é¢„çƒ­ {self.config.warmup_duration} ç§’...")
        warmup_tasks = [worker() for _ in range(min(concurrency, 10))]
        await asyncio.sleep(self.config.warmup_duration)
        
        # å–æ¶ˆé¢„çƒ­ä»»åŠ¡
        for task in warmup_tasks:
            task.cancel()
        
        # æ¸…ç†é¢„çƒ­ç»“æœ
        results.clear()
        error_types.clear()
        
        # æ­£å¼æµ‹è¯•é˜¶æ®µ
        print(f"  âš¡ æ­£å¼æµ‹è¯• {self.config.test_duration} ç§’...")
        start_test_time = time.time()
        
        # å¯åŠ¨å·¥ä½œåç¨‹
        workers = [worker() for _ in range(concurrency)]
        
        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        await asyncio.sleep(self.config.test_duration)
        
        # åœæ­¢æ‰€æœ‰å·¥ä½œåç¨‹
        for worker_task in workers:
            worker_task.cancel()
        
        # ç­‰å¾…æ‰€æœ‰åç¨‹å®Œæˆ
        await asyncio.gather(*workers, return_exceptions=True)
        
        actual_duration = time.time() - start_test_time
        
        # ç»Ÿè®¡ç»“æœ
        total_requests = len(results)
        successful_requests = sum(1 for r in results if r['success'])
        failed_requests = total_requests - successful_requests
        total_samples = sum(r['batch_size'] for r in results)
        
        if total_requests > 0:
            latencies = [r['latency'] for r in results if r['success']]
            avg_latency = statistics.mean(latencies) if latencies else 0
            p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) > 20 else 0
            p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) > 100 else 0
            throughput_rps = successful_requests / actual_duration
            throughput_sps = sum(r['batch_size'] for r in results if r['success']) / actual_duration
            success_rate = successful_requests / total_requests
        else:
            avg_latency = p95_latency = p99_latency = 0
            throughput_rps = throughput_sps = success_rate = 0
        
        result = TestResult(
            concurrency=concurrency,
            batch_size=batch_size,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            total_samples=total_samples,
            test_duration=actual_duration,
            avg_latency=avg_latency,
            p95_latency=p95_latency,
            p99_latency=p99_latency,
            throughput_rps=throughput_rps,
            throughput_sps=throughput_sps,
            success_rate=success_rate,
            error_types=error_types
        )
        
        # è¾“å‡ºç»“æœ
        print(f"  ğŸ“Š ç»“æœ: {successful_requests}/{total_requests} æˆåŠŸ, "
              f"{throughput_sps:.1f} samples/sec, "
              f"{avg_latency*1000:.1f}ms å¹³å‡å»¶è¿Ÿ")
        
        return result
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æ€§èƒ½æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹APIæ€§èƒ½æµ‹è¯•")
        print(f"ç›®æ ‡URL: {self.config.api_url}")
        print(f"æµ‹è¯•æ—¶é•¿: {self.config.test_duration}ç§’/é…ç½®")
        
        # å¥åº·æ£€æŸ¥
        try:
            async with self.session.get(f"{self.config.api_url}/health") as response:
                if response.status == 200:
                    health_info = await response.json()
                    print(f"âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡: {health_info.get('status', 'unknown')}")
                else:
                    print(f"âš ï¸  æœåŠ¡å¥åº·æ£€æŸ¥å¼‚å¸¸: HTTP {response.status}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡: {e}")
            return {}
        
        # æ‰§è¡Œæµ‹è¯•çŸ©é˜µ
        all_results = []
        
        for concurrency in self.config.concurrency_levels:
            for batch_size in self.config.batch_sizes:
                try:
                    result = await self.run_load_test(concurrency, batch_size)
                    all_results.append(result)
                    
                    # çŸ­æš‚ä¼‘æ¯é¿å…æœåŠ¡è¿‡è½½
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å¤±è´¥ (å¹¶å‘={concurrency}, æ‰¹æ¬¡={batch_size}): {e}")
        
        # åˆ†ææœ€ä¼˜é…ç½®
        best_config = self.analyze_results(all_results)
        
        return {
            "test_results": all_results,
            "best_configuration": best_config,
            "test_summary": self.generate_summary(all_results)
        }
    
    def analyze_results(self, results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœï¼Œæ‰¾å‡ºæœ€ä¼˜é…ç½®"""
        if not results:
            return {}
        
        # è¿‡æ»¤å‡ºæˆåŠŸç‡é«˜çš„ç»“æœ
        valid_results = [r for r in results if r.success_rate >= 0.95]
        
        if not valid_results:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æˆåŠŸç‡>=95%çš„é…ç½®")
            valid_results = results
        
        # æŒ‰ååé‡æ’åº
        best_throughput = max(valid_results, key=lambda x: x.throughput_sps)
        
        # æŒ‰å»¶è¿Ÿæ’åºï¼ˆæœ€ä½å»¶è¿Ÿï¼‰
        best_latency = min(valid_results, key=lambda x: x.avg_latency)
        
        # ç»¼åˆè¯„åˆ†ï¼ˆååé‡ * æˆåŠŸç‡ / å»¶è¿Ÿï¼‰
        def score(r: TestResult) -> float:
            return (r.throughput_sps * r.success_rate) / max(r.avg_latency, 0.001)
        
        best_overall = max(valid_results, key=score)
        
        return {
            "best_throughput": {
                "concurrency": best_throughput.concurrency,
                "batch_size": best_throughput.batch_size,
                "throughput_sps": best_throughput.throughput_sps,
                "success_rate": best_throughput.success_rate,
                "avg_latency": best_throughput.avg_latency
            },
            "best_latency": {
                "concurrency": best_latency.concurrency,
                "batch_size": best_latency.batch_size,
                "throughput_sps": best_latency.throughput_sps,
                "success_rate": best_latency.success_rate,
                "avg_latency": best_latency.avg_latency
            },
            "best_overall": {
                "concurrency": best_overall.concurrency,
                "batch_size": best_overall.batch_size,
                "throughput_sps": best_overall.throughput_sps,
                "success_rate": best_overall.success_rate,
                "avg_latency": best_overall.avg_latency,
                "score": score(best_overall)
            }
        }
    
    def generate_summary(self, results: List[TestResult]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        if not results:
            return {}
        
        max_throughput = max(results, key=lambda x: x.throughput_sps)
        min_latency = min(results, key=lambda x: x.avg_latency)
        
        return {
            "total_tests": len(results),
            "max_throughput": max_throughput.throughput_sps,
            "min_latency": min_latency.avg_latency,
            "avg_success_rate": statistics.mean([r.success_rate for r in results]),
            "total_samples_processed": sum([r.total_samples for r in results])
        }

def save_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_results = {}
    
    for key, value in results.items():
        if key == "test_results":
            serializable_results[key] = [
                {
                    "concurrency": r.concurrency,
                    "batch_size": r.batch_size,
                    "total_requests": r.total_requests,
                    "successful_requests": r.successful_requests,
                    "failed_requests": r.failed_requests,
                    "total_samples": r.total_samples,
                    "test_duration": r.test_duration,
                    "avg_latency": r.avg_latency,
                    "p95_latency": r.p95_latency,
                    "p99_latency": r.p99_latency,
                    "throughput_rps": r.throughput_rps,
                    "throughput_sps": r.throughput_sps,
                    "success_rate": r.success_rate,
                    "error_types": r.error_types
                }
                for r in value
            ]
        else:
            serializable_results[key] = value
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

async def main():
    parser = argparse.ArgumentParser(description='FastText APIæ€§èƒ½æµ‹è¯•å·¥å…·')
    parser.add_argument('--api-url', default='http://localhost:8000', help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--test-duration', type=int, default=30, help='æ¯ä¸ªé…ç½®çš„æµ‹è¯•æ—¶é•¿(ç§’)')
    parser.add_argument('--warmup-duration', type=int, default=5, help='é¢„çƒ­æ—¶é•¿(ç§’)')
    parser.add_argument('--output', default='api_performance_results.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼(å°‘é‡é…ç½®)')
    
    args = parser.parse_args()
    
    # é…ç½®æµ‹è¯•å‚æ•°
    if args.quick:
        concurrency_levels = [10, 30, 50]
        batch_sizes = [50, 200]
    else:
        concurrency_levels = [5, 10, 20, 30, 50, 80, 100, 150]
        batch_sizes = [10, 50, 100, 200, 500, 1000]
    
    config = TestConfig(
        api_url=args.api_url,
        test_duration=args.test_duration,
        warmup_duration=args.warmup_duration,
        concurrency_levels=concurrency_levels,
        batch_sizes=batch_sizes
    )
    
    # è¿è¡Œæµ‹è¯•
    async with APIPerformanceTester(config) as tester:
        results = await tester.run_comprehensive_test()
        
        if results:
            # è¾“å‡ºæœ€ä¼˜é…ç½®å»ºè®®
            best_config = results.get("best_configuration", {})
            if "best_overall" in best_config:
                best = best_config["best_overall"]
                print(f"\nğŸ† æ¨èé…ç½®:")
                print(f"  å¹¶å‘æ•°: {best['concurrency']}")
                print(f"  æ‰¹æ¬¡å¤§å°: {best['batch_size']}")
                print(f"  é¢„æœŸååé‡: {best['throughput_sps']:.1f} samples/sec")
                print(f"  å¹³å‡å»¶è¿Ÿ: {best['avg_latency']*1000:.1f}ms")
                print(f"  æˆåŠŸç‡: {best['success_rate']*100:.1f}%")
                print(f"\nğŸ“‹ CLIå‘½ä»¤å»ºè®®:")
                print(f"  --max-concurrent {best['concurrency']} --batch-size {best['batch_size']}")
            
            # ä¿å­˜ç»“æœ
            save_results(results, args.output)
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼Œæ— ç»“æœç”Ÿæˆ")

if __name__ == "__main__":
    asyncio.run(main())
