#!/usr/bin/env python3
"""
æ ‡ç­¾éªŒè¯è„šæœ¬ï¼šéªŒè¯FastTextæ¨ç†æ¡†æ¶çš„åˆ†ç±»æ­£ç¡®æ€§
éå†æ•´ä¸ªéªŒè¯é›†ï¼Œå¯¹æ¯”é¢„æµ‹æ ‡ç­¾ä¸çœŸå®æ ‡ç­¾ï¼Œè¾“å‡ºP/R/F1æŒ‡æ ‡
"""

import pandas as pd
import numpy as np
import asyncio
import aiohttp
import time
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from typing import List, Dict, Any

class LabelValidator:
    def __init__(self, service_url: str = "http://localhost:8000"):
        self.service_url = service_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def predict_batch(self, texts: List[str], batch_size: int = 50) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹æ–‡æœ¬"""
        if not texts:
            return []
        
        # é™åˆ¶æ‰¹æ¬¡å¤§å°é¿å…è¶…æ—¶
        batch_texts = texts[:batch_size]
        
        try:
            async with self.session.post(
                f"{self.service_url}/predict",
                json=batch_texts,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status}")
                    return [None for _ in batch_texts]
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¼‚å¸¸: {e}")
            return [None for _ in batch_texts]
    
    def parse_prediction(self, pred_result) -> str:
        """è§£æé¢„æµ‹ç»“æœï¼Œè¿”å›æ ‡ç­¾"""
        if not pred_result:
            return "__label__0"  # é»˜è®¤æ ‡ç­¾
            
        try:
            if isinstance(pred_result, list) and len(pred_result) >= 2:
                labels = pred_result[0]
                scores = pred_result[1]
                
                if labels and scores:
                    # å–å¾—åˆ†æœ€é«˜çš„
                    max_idx = np.argmax(scores)
                    raw_label = labels[max_idx]
                    
                    # è½¬æ¢æ ‡ç­¾æ ¼å¼
                    if raw_label.startswith('__label__'):
                        return raw_label
                    else:
                        return f"__label__{raw_label}"
        except Exception as e:
            print(f"âŒ è§£æé¢„æµ‹ç»“æœå¤±è´¥: {e}")
        
        return "__label__0"  # é»˜è®¤æ ‡ç­¾
    
    async def validate_all_labels(self, val_file: str, max_samples: int = None):
        """éªŒè¯æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾åˆ†ç±»æ­£ç¡®æ€§"""
        print(f"ğŸ“– è¯»å–éªŒè¯é›†: {val_file}")
        
        try:
            df = pd.read_parquet(val_file)
        except Exception as e:
            print(f"âŒ è¯»å–éªŒè¯é›†å¤±è´¥: {e}")
            return
        
        print(f"ğŸ“Š éªŒè¯é›†ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(df):,}")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {df['FT_label'].value_counts().to_dict()}")
        
        # é™åˆ¶æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_samples:
            df = df.head(max_samples)
            print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: åªéªŒè¯å‰ {len(df)} ä¸ªæ ·æœ¬")
        
        # è·å–å†…å®¹å’ŒçœŸå®æ ‡ç­¾
        contents = df['clean_content'].tolist()
        true_labels = df['FT_label'].tolist()
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹ {len(contents)} ä¸ªæ ·æœ¬...")
        start_time = time.time()
        
        # åˆ†æ‰¹é¢„æµ‹
        batch_size = 50
        all_predictions = []
        
        for i in range(0, len(contents), batch_size):
            batch_contents = contents[i:i+batch_size]
            batch_predictions = await self.predict_batch(batch_contents, batch_size)
            
            # è½¬æ¢é¢„æµ‹ç»“æœ
            batch_labels = [self.parse_prediction(pred) for pred in batch_predictions]
            all_predictions.extend(batch_labels)
            
            if (i // batch_size + 1) % 20 == 0:
                print(f"  å·²å¤„ç†: {i + len(batch_contents)}/{len(contents)} æ ·æœ¬")
        
        total_time = time.time() - start_time
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š ååé‡: {len(contents) / total_time:.1f} samples/sec")
        
        # æ˜¾ç¤ºè¯¦ç»†å¯¹æ¯”ä¿¡æ¯
        self.show_detailed_comparison(true_labels, all_predictions)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        self.calculate_metrics(true_labels, all_predictions)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        result_df = df.copy()
        result_df['predicted_label'] = all_predictions
        result_df['correct'] = result_df['FT_label'] == result_df['predicted_label']
        
        # æ˜¾ç¤ºæ ·æœ¬çº§åˆ«è¯¦ç»†åˆ†æ
        self.show_sample_analysis(result_df)
        
        # ä¿å­˜ç»“æœ
        output_file = f"label_validation_results_{int(time.time())}.parquet"
        result_df.to_parquet(output_file)
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            'total_samples': len(contents),
            'correct_predictions': sum(result_df['correct']),
            'accuracy': sum(result_df['correct']) / len(contents),
            'processing_time': total_time,
            'throughput': len(contents) / total_time
        }
    
    def show_detailed_comparison(self, true_labels: List[str], pred_labels: List[str]):
        """æ˜¾ç¤ºè¯¦ç»†çš„æ ‡ç­¾å¯¹æ¯”ä¿¡æ¯"""
        print(f"\nğŸ” æ ‡ç­¾æ ¼å¼å¯¹æ¯”:")
        print("=" * 80)
        
        # æ˜¾ç¤ºçœŸå®æ ‡ç­¾æ ¼å¼
        true_unique = sorted(set(true_labels))
        print(f"ç®—æ³•éªŒè¯é›†æ ‡ç­¾æ ¼å¼: {true_unique}")
        
        # æ˜¾ç¤ºæˆ‘ä»¬çš„é¢„æµ‹æ ¼å¼  
        pred_unique = sorted(set(pred_labels))
        print(f"æˆ‘ä»¬æœåŠ¡é¢„æµ‹æ ¼å¼: {pred_unique}")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”
        from collections import Counter
        true_counts = Counter(true_labels)
        pred_counts = Counter(pred_labels)
        
        print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”:")
        print(f"{'æ ‡ç­¾':<15} {'ç®—æ³•éªŒè¯é›†':<10} {'æˆ‘ä»¬é¢„æµ‹':<10} {'å·®å¼‚':<10}")
        print("-" * 50)
        for label in true_unique:
            true_count = true_counts.get(label, 0)
            pred_count = pred_counts.get(label, 0)
            diff = pred_count - true_count
            print(f"{label:<15} {true_count:<10} {pred_count:<10} {diff:+<10}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†å¯¹æ¯”
        print(f"\nğŸ‘€ å‰10ä¸ªæ ·æœ¬å¯¹æ¯”:")
        print(f"{'æ ·æœ¬':<6} {'ç®—æ³•æ ‡ç­¾':<15} {'æˆ‘ä»¬é¢„æµ‹':<15} {'åŒ¹é…':<6}")
        print("-" * 50)
        for i in range(min(10, len(true_labels))):
            match = "âœ…" if true_labels[i] == pred_labels[i] else "âŒ"
            print(f"{i+1:<6} {true_labels[i]:<15} {pred_labels[i]:<15} {match:<6}")
    
    def show_sample_analysis(self, result_df: pd.DataFrame):
        """æ˜¾ç¤ºæ ·æœ¬çº§åˆ«çš„è¯¦ç»†åˆ†æ"""
        print(f"\nğŸ“ˆ é”™è¯¯æ ·æœ¬è¯¦ç»†åˆ†æ:")
        print("=" * 80)
        
        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_samples = result_df[~result_df['correct']]
        if len(error_samples) > 0:
            print(f"æ€»é”™è¯¯æ ·æœ¬: {len(error_samples)}/{len(result_df)} ({len(error_samples)/len(result_df)*100:.1f}%)")
            
            error_stats = error_samples.groupby(['FT_label', 'predicted_label']).size()
            print(f"\né”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for (true_label, pred_label), count in error_stats.items():
                pct = count / len(error_samples) * 100
                print(f"  {true_label} â†’ {pred_label}: {count} ä¸ªæ ·æœ¬ ({pct:.1f}%)")
            
            # æ˜¾ç¤ºå‡ ä¸ªå…·ä½“çš„é”™è¯¯æ ·æœ¬
            print(f"\nğŸ” é”™è¯¯æ ·æœ¬ç¤ºä¾‹ (å‰5ä¸ª):")
            error_examples = error_samples.head(5)
            for idx, row in error_examples.iterrows():
                content_preview = row['content'][:100] + "..." if len(row['content']) > 100 else row['content']
                print(f"\næ ·æœ¬ {idx}:")
                print(f"  å†…å®¹: {content_preview}")
                print(f"  çœŸå®æ ‡ç­¾: {row['FT_label']}")
                print(f"  é¢„æµ‹æ ‡ç­¾: {row['predicted_label']}")
                print(f"  å†…å®¹é•¿åº¦: {len(row['content'])} å­—ç¬¦")
        else:
            print(f"ğŸ‰ æ‰€æœ‰æ ·æœ¬éƒ½é¢„æµ‹æ­£ç¡®ï¼")
    
    def calculate_metrics(self, true_labels: List[str], pred_labels: List[str]):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print(f"\nğŸ“Š æ ‡ç­¾éªŒè¯ç»“æœ:")
        print("=" * 80)
        
        # å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, pred_labels)
        print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            true_labels, 
            pred_labels, 
            target_names=['__label__0', '__label__1'],
            digits=4
        )
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(report)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, pred_labels)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"             é¢„æµ‹")
        print(f"çœŸå®    __label__0  __label__1")
        print(f"__label__0    {cm[0,0]:8d}    {cm[0,1]:8d}")
        print(f"__label__1    {cm[1,0]:8d}    {cm[1,1]:8d}")
        
        # ä¸åŸºå‡†å¯¹æ¯”
        print(f"\nğŸ¯ ä¸åŸºå‡†æ€§èƒ½å¯¹æ¯”:")
        print(f"åŸºå‡† __label__0: P=0.9902, R=0.8902, F1=0.9375")
        print(f"åŸºå‡† __label__1: P=0.6466, R=0.9579, F1=0.7721")
        print(f"åŸºå‡†æ•´ä½“: P=0.9023, R=0.9018, F1=0.9021")
        
        # é—®é¢˜è¯Šæ–­æç¤º
        print(f"\nğŸ”§ é—®é¢˜è¯Šæ–­æç¤º:")
        if accuracy < 0.5:
            print(f"  å‡†ç¡®ç‡ < 50%ï¼Œå¯èƒ½æ˜¯æ ‡ç­¾æ˜ å°„é—®é¢˜")
        elif accuracy < 0.85:
            print(f"  å‡†ç¡®ç‡ < 85%ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æˆ–é¢„å¤„ç†é—®é¢˜")
        else:
            print(f"  å‡†ç¡®ç‡ > 85%ï¼Œæ¡†æ¶åŸºæœ¬æ­£å¸¸")
        
        print(f"\nğŸ’¡ ä¸ç®—æ³•åŒå­¦è®¨è®ºè¦ç‚¹:")
        print(f"  1. ç¡®è®¤æœåŠ¡è¿”å›çš„ '0'/'1' ä¸ '__label__0'/'__label__1' çš„å¯¹åº”å…³ç³»")
        print(f"  2. ç¡®è®¤æ¨¡å‹æ–‡ä»¶æ˜¯å¦ä¸éªŒè¯é›†åŒ¹é…")
        print(f"  3. ç¡®è®¤æ–‡æœ¬é¢„å¤„ç†æ˜¯å¦ä¸€è‡´")
        print(f"  4. ç¡®è®¤éªŒè¯é›†çš„æ ‡ç­¾å®šä¹‰ (__label__0=ä½è´¨é‡? __label__1=é«˜è´¨é‡?)")

async def main():
    import argparse
    parser = argparse.ArgumentParser(description='FastTextæ ‡ç­¾éªŒè¯è„šæœ¬')
    parser.add_argument('--service-url', 
                        default='http://localhost:8000',
                        help='FastTextæœåŠ¡URL')
    parser.add_argument('--val-file',
                        default='/mnt/project/yifan/data/code/the-stack-v2_fasttext_val_with_gt.parquet',
                        help='éªŒè¯é›†æ–‡ä»¶')
    parser.add_argument('--max-samples', 
                        type=int,
                        help='æœ€å¤§éªŒè¯æ ·æœ¬æ•°ï¼ˆä¸æŒ‡å®šåˆ™éªŒè¯å…¨éƒ¨ï¼‰')
    
    args = parser.parse_args()
    
    async with LabelValidator(args.service_url) as validator:
        await validator.validate_all_labels(args.val_file, args.max_samples)

if __name__ == "__main__":
    asyncio.run(main())
