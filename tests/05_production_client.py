#!/usr/bin/env python3
"""
ç”Ÿäº§çº§æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯
ä¸“é—¨ç”¨äºå¤„ç†the-stack-v2æ•°æ®çš„å¤§è§„æ¨¡FastTextæ¨ç†
"""
import asyncio
import aiohttp
import pandas as pd
import numpy as np
import json
import time
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Iterator, Optional, Tuple
import argparse
from dataclasses import dataclass

@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½®"""
    data_dir: str
    output_dir: str
    service_url: str
    batch_size: int = 100
    max_concurrent: int = 10
    max_text_length: int = 100000  # æœ€å¤§æ–‡æœ¬é•¿åº¦
    timeout: int = 120
    skip_corrupted: bool = True
    save_format: str = "parquet"  # parquet æˆ– json
    resume_from_checkpoint: bool = True  # æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­

class TheStackV2Processor:
    """The-Stack-v2 æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.processed_count = 0
        self.error_count = 0
        self.start_time = None
        self.progress_file = Path(config.output_dir) / "progress.json"
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        connector = aiohttp.TCPConnector(limit=self.config.max_concurrent * 2)
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def discover_parquet_files(self) -> List[Path]:
        """å‘ç°æ‰€æœ‰parquetæ–‡ä»¶"""
        print(f"ğŸ” æ‰«ææ•°æ®ç›®å½•: {self.config.data_dir}")
        
        data_path = Path(self.config.data_dir)
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.config.data_dir}")
        
        parquet_files = list(data_path.glob("*.parquet"))
        print(f"ğŸ“ å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
        
        # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼šå°æ–‡ä»¶ä¼˜å…ˆï¼Œä¾¿äºå¿«é€ŸéªŒè¯å’Œè´Ÿè½½å‡è¡¡
        parquet_files.sort(key=lambda f: f.stat().st_size)
        print(f"ğŸ“Š æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå°åˆ°å¤§ï¼‰")
        
        return parquet_files
    
    def load_progress(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†è¿›åº¦"""
        if not self.progress_file.exists():
            return {"completed_files": [], "start_time": None}
        
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
            print(f"ğŸ“‚ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå·²å®Œæˆ {len(progress.get('completed_files', []))} ä¸ªæ–‡ä»¶")
            return progress
        except Exception as e:
            print(f"âš ï¸ æ–­ç‚¹æ–‡ä»¶æŸåï¼Œä»å¤´å¼€å§‹: {e}")
            return {"completed_files": [], "start_time": None}
    
    def save_progress(self, completed_files: List[str], stats: Dict[str, Any] = None):
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.progress_file.parent.mkdir(parents=True, exist_ok=True)
            
            progress = {
                "completed_files": completed_files,
                "start_time": pd.Timestamp.fromtimestamp(self.start_time).isoformat() if self.start_time else None,
                "last_update": pd.Timestamp.now().isoformat(),
                "total_processed": self.processed_count,
                "total_errors": self.error_count,
                "stats": stats or {}
            }
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def filter_remaining_files(self, all_files: List[Path], test_mode: bool = False) -> List[Path]:
        """è¿‡æ»¤å‡ºå‰©ä½™æœªå¤„ç†çš„æ–‡ä»¶"""
        # æµ‹è¯•æ¨¡å¼ï¼šé€‰æ‹©å·²çŸ¥çš„å¥½æ–‡ä»¶è¿›è¡Œæµ‹è¯•
        if test_mode:
            # å¯»æ‰¾ä¹‹å‰æµ‹è¯•è¿‡çš„å¥½æ–‡ä»¶
            known_good_files = [
                "the-stack-v2-dedup-train-00516-of-01078.parquet",
                "the-stack-v2-dedup-train-00341-of-01078.parquet", 
                "the-stack-v2-dedup-train-00195-of-01078.parquet"
            ]
            
            test_files = []
            for file_path in all_files:
                if file_path.name in known_good_files:
                    test_files.append(file_path)
                    if len(test_files) >= 2:  # åªè¦2ä¸ªæ–‡ä»¶æµ‹è¯•
                        break
            
            if not test_files:
                # å¦‚æœæ‰¾ä¸åˆ°å·²çŸ¥å¥½æ–‡ä»¶ï¼Œé€‰æ‹©ä¸­ç­‰å¤§å°çš„æ–‡ä»¶ï¼ˆé¿å…æœ€å°çš„æŸåæ–‡ä»¶ï¼‰
                mid_point = len(all_files) // 2
                test_files = all_files[mid_point:mid_point+2]
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šæœªæ‰¾åˆ°å·²çŸ¥å¥½æ–‡ä»¶ï¼Œé€‰æ‹©ä¸­ç­‰å¤§å°çš„æ–‡ä»¶")
            else:
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé€‰æ‹©å·²çŸ¥çš„å¥½æ–‡ä»¶è¿›è¡Œæµ‹è¯•")
            
            print(f"ğŸ“‹ æµ‹è¯•æ–‡ä»¶: {[f.name for f in test_files]}")
            return test_files
        
        if not self.config.resume_from_checkpoint:
            return all_files
        
        progress = self.load_progress()
        completed_files = set(progress.get("completed_files", []))
        
        remaining_files = [f for f in all_files if f.name not in completed_files]
        
        if len(remaining_files) < len(all_files):
            skipped_count = len(all_files) - len(remaining_files)
            print(f"ğŸ”„ æ–­ç‚¹ç»§ç»­ï¼šè·³è¿‡å·²å®Œæˆçš„ {skipped_count} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(remaining_files)} ä¸ª")
        
        return remaining_files
    
    def load_parquet_safe(self, file_path: Path) -> Optional[pd.DataFrame]:
        """å®‰å…¨åŠ è½½parquetæ–‡ä»¶"""
        try:
            df = pd.read_parquet(file_path, engine='pyarrow')
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'content' not in df.columns:
                print(f"âš ï¸ {file_path.name}: ç¼ºå°‘'content'åˆ—")
                return None
            
            print(f"âœ… {file_path.name}: æˆåŠŸåŠ è½½ {len(df)} è¡Œ")
            return df
            
        except Exception as e:
            if self.config.skip_corrupted:
                print(f"âš ï¸ {file_path.name}: è·³è¿‡æŸåæ–‡ä»¶ - {e}")
                return None
            else:
                raise e
    
    def preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†contentå†…å®¹"""
        if pd.isna(content):
            return ""
        
        content_str = str(content)
        
        # å¯é€‰ï¼šä¿ç•™åŸå§‹é•¿åº¦ç”¨äºåç»­åˆ†æ
        # ä¸è¿›è¡Œæˆªæ–­ï¼Œè®©æ¨¡å‹å¤„ç†å®Œæ•´å†…å®¹
        return content_str
    
    async def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        if not self.session:
            raise RuntimeError("Session not initialized")
        
        url = f"{self.config.service_url}/predict"
        params = {"k": 2, "threshold": 0.0}
        
        try:
            async with self.session.post(url, json=texts, params=params) as response:
                if response.status == 200:
                    results = await response.json()
                    
                    # è½¬æ¢æ ¼å¼ [(labels, scores)] -> [{"labels": [], "scores": [], ...}]
                    formatted_results = []
                    for labels, scores in results:
                        formatted_results.append({
                            "labels": labels,
                            "scores": scores,
                            "prediction": labels[0] if labels else "unknown",
                            "confidence": scores[0] if scores else 0.0
                        })
                    
                    return formatted_results
                else:
                    error_text = await response.text()
                    raise RuntimeError(f"HTTP {response.status}: {error_text}")
                    
        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return [
                {
                    "labels": ["error"],
                    "scores": [0.0],
                    "prediction": "error",
                    "confidence": 0.0
                } for _ in texts
            ]
    
    def postprocess_results(self, original_data: List[Dict], predictions: List[Dict]) -> List[Dict]:
        """åå¤„ç†ç»“æœ"""
        results = []
        
        for original, prediction in zip(original_data, predictions):
            # æ£€æŸ¥é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§
            is_valid, invalid_reason = self.validate_prediction(prediction)
            
            # æ„å»ºè¾“å‡ºè®°å½•ï¼šä¿ç•™å…³é”®å­—æ®µ + é¢„æµ‹ç»“æœ
            result = {
                # æºæ–‡ä»¶å…³è”å­—æ®µï¼ˆç”¨äºåç»­åŒ¹é…ï¼‰
                "blob_id": original.get("blob_id"),
                "path": original.get("path"),
                "repo_name": original.get("repo_name"),
                "language": original.get("language"),
                "filename": original.get("filename"),
                "length_bytes": original.get("length_bytes"),
                
                # é¢„æµ‹ç»“æœå­—æ®µ
                "quality_labels": prediction["labels"],
                "quality_scores": prediction["scores"], 
                "primary_prediction": prediction["prediction"],  # ä¸»è¦é¢„æµ‹ç±»åˆ«
                "primary_confidence": prediction["confidence"],  # ä¸»è¦ç½®ä¿¡åº¦
                
                # æ•°æ®æœ‰æ•ˆæ€§å­—æ®µ
                "valid": is_valid,
                "invalid_reason": invalid_reason if not is_valid else None,
                
                # å¤„ç†å…ƒä¿¡æ¯
                "content_length": len(str(original.get("content", ""))),
                "processed_at": pd.Timestamp.now().isoformat()
            }
            
            results.append(result)
        
        return results
    
    def validate_prediction(self, prediction: Dict) -> Tuple[bool, Optional[str]]:
        """éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§"""
        try:
            labels = prediction.get("labels", [])
            scores = prediction.get("scores", [])
            
            # æ£€æŸ¥æ ‡ç­¾æ•°é‡æ˜¯å¦æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯2ä¸ªï¼š'0'å’Œ'1'ï¼‰
            if len(labels) != 2:
                return False, f"unexpected_label_count:{len(labels)}"
            
            # æ£€æŸ¥åˆ†æ•°æ•°é‡æ˜¯å¦åŒ¹é…
            if len(scores) != len(labels):
                return False, f"label_score_mismatch:{len(labels)}vs{len(scores)}"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„æ ‡ç­¾
            expected_labels = {"0", "1"}
            if set(labels) != expected_labels:
                return False, f"unexpected_labels:{labels}"
            
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦åˆç†
            if not all(0 <= score <= 1 for score in scores):
                return False, f"invalid_scores:{scores}"
            
            return True, None
            
        except Exception as e:
            return False, f"validation_error:{str(e)}"
    
    async def process_file(self, file_path: Path, semaphore: asyncio.Semaphore) -> Tuple[int, int]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        async with semaphore:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # åŠ è½½æ•°æ®
            df = self.load_parquet_safe(file_path)
            if df is None:
                return 0, 0
            
            if len(df) == 0:
                print(f"âš ï¸ {file_path.name}: ç©ºæ–‡ä»¶")
                return 0, 0
            
            # æå–å’Œé¢„å¤„ç†å†…å®¹
            original_data = []
            texts_for_prediction = []
            
            for idx, row in df.iterrows():
                content = self.preprocess_content(row['content'])
                if content:
                    # ä¿å­˜åŸå§‹æ•°æ®
                    row_dict = row.to_dict()
                    original_data.append(row_dict)
                    texts_for_prediction.append(content)
            
            if not texts_for_prediction:
                print(f"âš ï¸ {file_path.name}: æ²¡æœ‰æœ‰æ•ˆå†…å®¹")
                return 0, 0
            
            print(f"ğŸ“Š {file_path.name}: å‡†å¤‡å¤„ç† {len(texts_for_prediction)} ä¸ªæ ·æœ¬")
            
            # åˆ†æ‰¹å¤„ç†
            all_results = []
            batch_count = 0
            
            for i in range(0, len(texts_for_prediction), self.config.batch_size):
                batch_texts = texts_for_prediction[i:i + self.config.batch_size]
                batch_original = original_data[i:i + self.config.batch_size]
                
                batch_count += 1
                print(f"  æ‰¹æ¬¡ {batch_count}: {len(batch_texts)} æ ·æœ¬")
                
                # é¢„æµ‹
                predictions = await self.predict_batch(batch_texts)
                
                # åå¤„ç†
                batch_results = self.postprocess_results(batch_original, predictions)
                all_results.extend(batch_results)
            
            # ä¿å­˜ç»“æœ
            success_count = await self.save_results(file_path, all_results)
            
            print(f"âœ… {file_path.name}: å®Œæˆ {success_count} ä¸ªæ ·æœ¬")
            return success_count, len(texts_for_prediction) - success_count
    
    async def save_results(self, source_file: Path, results: List[Dict]) -> int:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        if not results:
            return 0
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        base_name = source_file.stem  # ä¸å«æ‰©å±•å
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        total_count = len(results)
        valid_count = sum(1 for r in results if r.get("valid", True))
        invalid_count = total_count - valid_count
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = {
            "source_file": source_file.name,
            "total_samples": total_count,
            "valid_samples": valid_count,
            "invalid_samples": invalid_count,
            "invalid_rate": invalid_count / total_count if total_count > 0 else 0,
            "processing_time": pd.Timestamp.now().isoformat()
        }
        
        if self.config.save_format == "parquet":
            # ä¸»è¦ç»“æœæ–‡ä»¶
            output_file = output_dir / f"{base_name}_quality_predictions.parquet"
            try:
                df = pd.DataFrame(results)
                df.to_parquet(output_file, engine='pyarrow', index=False)
                print(f"ğŸ’¾ é¢„æµ‹ç»“æœ: {output_file}")
                
                # å¯é€‰ï¼šå¦‚æœéœ€è¦å¿«é€ŸæŸ¥çœ‹ç»Ÿè®¡ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢å‡ è¡Œ
                # stats_file = output_dir / f"{base_name}_stats.json"
                # with open(stats_file, 'w', encoding='utf-8') as f:
                #     json.dump(stats, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬: {valid_count}/{total_count} ({valid_count/total_count*100:.1f}%)")
                
                return valid_count
            except Exception as e:
                print(f"âŒ ä¿å­˜parquetå¤±è´¥: {e}")
                return 0
        
        elif self.config.save_format == "json":
            output_file = output_dir / f"{base_name}_quality_predictions.json"
            try:
                output_data = {
                    "stats": stats,
                    "predictions": results
                }
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
                return valid_count
            except Exception as e:
                print(f"âŒ ä¿å­˜jsonå¤±è´¥: {e}")
                return 0
        
        else:
            print(f"âŒ ä¸æ”¯æŒçš„ä¿å­˜æ ¼å¼: {self.config.save_format}")
            return 0
    
    async def process_all_files(self, test_mode: bool = False) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹å¤§è§„æ¨¡æ•°æ®å¤„ç†")
        print("=" * 50)
        
        self.start_time = time.time()
        
        # å‘ç°æ–‡ä»¶
        all_files = self.discover_parquet_files()
        
        if not all_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°parquetæ–‡ä»¶")
            return {"status": "no_files"}
        
        # è¿‡æ»¤å‡ºå‰©ä½™æœªå¤„ç†çš„æ–‡ä»¶
        remaining_files = self.filter_remaining_files(all_files, test_mode)
        
        if not remaining_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
            return {"status": "already_completed"}
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        # å¤„ç†æ–‡ä»¶
        print(f"âš¡ å¼€å§‹å¤„ç† {len(remaining_files)} ä¸ªæ–‡ä»¶ï¼ˆå…±{len(all_files)}ä¸ªï¼‰")
        print(f"ğŸ“Š å¹¶å‘æ•°: {self.config.max_concurrent}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        
        completed_files = []
        total_success = 0
        total_errors = 0
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶ä»¥æ”¯æŒæ–­ç‚¹ä¿å­˜
        for i, file_path in enumerate(remaining_files):
            print(f"\nğŸ“Š è¿›åº¦: {i+1}/{len(remaining_files)} - {file_path.name}")
            
            try:
                result = await self.process_file(file_path, semaphore)
                if isinstance(result, Exception):
                    print(f"âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {result}")
                    total_errors += 1
                else:
                    success, errors = result
                    total_success += success
                    total_errors += errors
                    
                    # æ ‡è®°ä¸ºå·²å®Œæˆ
                    completed_files.append(file_path.name)
                    
                    # ä¿å­˜è¿›åº¦
                    self.save_progress(completed_files, {
                        "current_file": i + 1,
                        "total_files": len(remaining_files),
                        "success_samples": total_success,
                        "error_samples": total_errors
                    })
                    
                    print(f"âœ… å·²å®Œæˆ: {len(completed_files)}/{len(remaining_files)}")
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å¼‚å¸¸: {e}")
                total_errors += 1
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
                continue
        
        elapsed = time.time() - self.start_time
        throughput = total_success / elapsed if elapsed > 0 else 0
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"  å¤„ç†æ–‡ä»¶æ•°: {len(completed_files)}/{len(remaining_files)}")
        print(f"  æˆåŠŸæ ·æœ¬: {total_success:,}")
        print(f"  å¤±è´¥æ ·æœ¬: {total_errors:,}")
        print(f"  æ€»è€—æ—¶: {elapsed:.1f} ç§’")
        print(f"  ååé‡: {throughput:.0f} samples/sec")
        
        return {
            "status": "completed",
            "processed_files": len(completed_files),
            "total_files": len(all_files),
            "successful_samples": total_success,
            "failed_samples": total_errors,
            "total_time": elapsed,
            "throughput": throughput
        }

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="The-Stack-v2 æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯")
    
    parser.add_argument("--data-dir", required=True,
                       help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir", required=True,
                       help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--service-url", required=True,
                       help="FastTextæœåŠ¡URL")
    parser.add_argument("--batch-size", type=int, default=100,
                       help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 100)")
    parser.add_argument("--max-concurrent", type=int, default=10,
                       help="æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 10)")
    parser.add_argument("--max-text-length", type=int, default=100000,
                       help="æœ€å¤§æ–‡æœ¬é•¿åº¦ (é»˜è®¤: 100000)")
    parser.add_argument("--timeout", type=int, default=120,
                       help="è¯·æ±‚è¶…æ—¶æ—¶é—´ (é»˜è®¤: 120ç§’)")
    parser.add_argument("--save-format", choices=["parquet", "json"], default="parquet",
                       help="ä¿å­˜æ ¼å¼ (é»˜è®¤: parquet)")
    parser.add_argument("--no-skip-corrupted", action="store_true",
                       help="ä¸è·³è¿‡æŸåçš„æ–‡ä»¶")
    parser.add_argument("--no-resume", action="store_true",
                       help="ä¸ä»æ–­ç‚¹ç»§ç»­ï¼Œé‡æ–°å¼€å§‹å¤„ç†")
    parser.add_argument("--test-mode", action="store_true", 
                       help="æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰2ä¸ªæœ€å°æ–‡ä»¶")
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    config = ProcessingConfig(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        service_url=args.service_url,
        batch_size=args.batch_size,
        max_concurrent=args.max_concurrent,
        max_text_length=args.max_text_length,
        timeout=args.timeout,
        skip_corrupted=not args.no_skip_corrupted,
        save_format=args.save_format,
        resume_from_checkpoint=not args.no_resume
    )
    
    print("ğŸ¯ ç”Ÿäº§çº§æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯")
    print("=" * 50)
    print(f"æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"æœåŠ¡åœ°å€: {config.service_url}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"å¹¶å‘æ•°: {config.max_concurrent}")
    print(f"ä¿å­˜æ ¼å¼: {config.save_format}")
    print(f"æ–­ç‚¹ç»§ç»­: {config.resume_from_checkpoint}")
    if args.test_mode:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰2ä¸ªæœ€å°æ–‡ä»¶")
    print()
    
    try:
        async with TheStackV2Processor(config) as processor:
            result = await processor.process_all_files(test_mode=args.test_mode)
            
            # ä¿å­˜å¤„ç†æŠ¥å‘Š
            report_file = Path(config.output_dir) / "processing_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
