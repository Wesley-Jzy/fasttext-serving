#!/usr/bin/env python3
"""
ç”Ÿäº§çº§æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯
ä¸“é—¨ç”¨äºå¤„ç†the-stack-v2æ•°æ®çš„å¤§è§„æ¨¡FastTextæ¨ç†
"""
import asyncio
import aiohttp
import pandas as pd
import numpy as np
import json
import time
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Iterator, Optional, Tuple
import argparse
from dataclasses import dataclass

@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½®"""
    data_dir: str
    output_dir: str
    service_url: str
    batch_size: int = 100
    max_concurrent: int = 10
    max_text_length: int = 100000  # æœ€å¤§æ–‡æœ¬é•¿åº¦
    timeout: int = 120
    skip_corrupted: bool = True
    save_format: str = "parquet"  # parquet æˆ– json

class TheStackV2Processor:
    """The-Stack-v2 æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.processed_count = 0
        self.error_count = 0
        self.start_time = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        connector = aiohttp.TCPConnector(limit=self.config.max_concurrent * 2)
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def discover_parquet_files(self) -> List[Path]:
        """å‘ç°æ‰€æœ‰parquetæ–‡ä»¶"""
        print(f"ğŸ” æ‰«ææ•°æ®ç›®å½•: {self.config.data_dir}")
        
        data_path = Path(self.config.data_dir)
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.config.data_dir}")
        
        parquet_files = list(data_path.glob("*.parquet"))
        print(f"ğŸ“ å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        parquet_files.sort()
        
        return parquet_files
    
    def load_parquet_safe(self, file_path: Path) -> Optional[pd.DataFrame]:
        """å®‰å…¨åŠ è½½parquetæ–‡ä»¶"""
        try:
            df = pd.read_parquet(file_path, engine='pyarrow')
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'content' not in df.columns:
                print(f"âš ï¸ {file_path.name}: ç¼ºå°‘'content'åˆ—")
                return None
            
            print(f"âœ… {file_path.name}: æˆåŠŸåŠ è½½ {len(df)} è¡Œ")
            return df
            
        except Exception as e:
            if self.config.skip_corrupted:
                print(f"âš ï¸ {file_path.name}: è·³è¿‡æŸåæ–‡ä»¶ - {e}")
                return None
            else:
                raise e
    
    def preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†contentå†…å®¹"""
        if pd.isna(content):
            return ""
        
        content_str = str(content)
        
        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        if len(content_str) > self.config.max_text_length:
            print(f"âš ï¸ æˆªæ–­é•¿æ–‡æœ¬: {len(content_str)} -> {self.config.max_text_length}")
            content_str = content_str[:self.config.max_text_length]
        
        return content_str
    
    async def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        if not self.session:
            raise RuntimeError("Session not initialized")
        
        url = f"{self.config.service_url}/predict"
        params = {"k": 2, "threshold": 0.0}
        
        try:
            async with self.session.post(url, json=texts, params=params) as response:
                if response.status == 200:
                    results = await response.json()
                    
                    # è½¬æ¢æ ¼å¼ [(labels, scores)] -> [{"labels": [], "scores": [], ...}]
                    formatted_results = []
                    for labels, scores in results:
                        formatted_results.append({
                            "labels": labels,
                            "scores": scores,
                            "prediction": labels[0] if labels else "unknown",
                            "confidence": scores[0] if scores else 0.0
                        })
                    
                    return formatted_results
                else:
                    error_text = await response.text()
                    raise RuntimeError(f"HTTP {response.status}: {error_text}")
                    
        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return [
                {
                    "labels": ["error"],
                    "scores": [0.0],
                    "prediction": "error",
                    "confidence": 0.0
                } for _ in texts
            ]
    
    def postprocess_results(self, original_data: List[Dict], predictions: List[Dict]) -> List[Dict]:
        """åå¤„ç†ç»“æœ"""
        results = []
        
        for original, prediction in zip(original_data, predictions):
            result = original.copy()
            result.update({
                "quality_prediction": prediction["prediction"],
                "quality_confidence": prediction["confidence"],
                "quality_labels": prediction["labels"],
                "quality_scores": prediction["scores"]
            })
            results.append(result)
        
        return results
    
    async def process_file(self, file_path: Path, semaphore: asyncio.Semaphore) -> Tuple[int, int]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        async with semaphore:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # åŠ è½½æ•°æ®
            df = self.load_parquet_safe(file_path)
            if df is None:
                return 0, 0
            
            if len(df) == 0:
                print(f"âš ï¸ {file_path.name}: ç©ºæ–‡ä»¶")
                return 0, 0
            
            # æå–å’Œé¢„å¤„ç†å†…å®¹
            original_data = []
            texts_for_prediction = []
            
            for idx, row in df.iterrows():
                content = self.preprocess_content(row['content'])
                if content:
                    # ä¿å­˜åŸå§‹æ•°æ®
                    row_dict = row.to_dict()
                    original_data.append(row_dict)
                    texts_for_prediction.append(content)
            
            if not texts_for_prediction:
                print(f"âš ï¸ {file_path.name}: æ²¡æœ‰æœ‰æ•ˆå†…å®¹")
                return 0, 0
            
            print(f"ğŸ“Š {file_path.name}: å‡†å¤‡å¤„ç† {len(texts_for_prediction)} ä¸ªæ ·æœ¬")
            
            # åˆ†æ‰¹å¤„ç†
            all_results = []
            batch_count = 0
            
            for i in range(0, len(texts_for_prediction), self.config.batch_size):
                batch_texts = texts_for_prediction[i:i + self.config.batch_size]
                batch_original = original_data[i:i + self.config.batch_size]
                
                batch_count += 1
                print(f"  æ‰¹æ¬¡ {batch_count}: {len(batch_texts)} æ ·æœ¬")
                
                # é¢„æµ‹
                predictions = await self.predict_batch(batch_texts)
                
                # åå¤„ç†
                batch_results = self.postprocess_results(batch_original, predictions)
                all_results.extend(batch_results)
            
            # ä¿å­˜ç»“æœ
            success_count = await self.save_results(file_path, all_results)
            
            print(f"âœ… {file_path.name}: å®Œæˆ {success_count} ä¸ªæ ·æœ¬")
            return success_count, len(texts_for_prediction) - success_count
    
    async def save_results(self, source_file: Path, results: List[Dict]) -> int:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        if not results:
            return 0
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        base_name = source_file.stem  # ä¸å«æ‰©å±•å
        
        if self.config.save_format == "parquet":
            output_file = output_dir / f"{base_name}_processed.parquet"
            try:
                df = pd.DataFrame(results)
                df.to_parquet(output_file, engine='pyarrow', index=False)
                print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
                return len(results)
            except Exception as e:
                print(f"âŒ ä¿å­˜parquetå¤±è´¥: {e}")
                return 0
        
        elif self.config.save_format == "json":
            output_file = output_dir / f"{base_name}_processed.json"
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
                return len(results)
            except Exception as e:
                print(f"âŒ ä¿å­˜jsonå¤±è´¥: {e}")
                return 0
        
        else:
            print(f"âŒ ä¸æ”¯æŒçš„ä¿å­˜æ ¼å¼: {self.config.save_format}")
            return 0
    
    async def process_all_files(self) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹å¤§è§„æ¨¡æ•°æ®å¤„ç†")
        print("=" * 50)
        
        self.start_time = time.time()
        
        # å‘ç°æ–‡ä»¶
        parquet_files = self.discover_parquet_files()
        
        if not parquet_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°parquetæ–‡ä»¶")
            return {"status": "no_files"}
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        # å¹¶å‘å¤„ç†æ–‡ä»¶
        print(f"âš¡ å¼€å§‹å¹¶å‘å¤„ç† {len(parquet_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“Š å¹¶å‘æ•°: {self.config.max_concurrent}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        
        tasks = [
            self.process_file(file_path, semaphore)
            for file_path in parquet_files
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        total_success = 0
        total_errors = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ æ–‡ä»¶ {parquet_files[i].name} å¤„ç†å¼‚å¸¸: {result}")
                total_errors += 1
            else:
                success, errors = result
                total_success += success
                total_errors += errors
        
        elapsed = time.time() - self.start_time
        throughput = total_success / elapsed if elapsed > 0 else 0
        
        # è¾“å‡ºç»Ÿè®¡
        print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(parquet_files)}")
        print(f"  æˆåŠŸæ ·æœ¬: {total_success:,}")
        print(f"  å¤±è´¥æ ·æœ¬: {total_errors:,}")
        print(f"  æ€»è€—æ—¶: {elapsed:.1f} ç§’")
        print(f"  ååé‡: {throughput:.0f} samples/sec")
        
        return {
            "status": "completed",
            "total_files": len(parquet_files),
            "successful_samples": total_success,
            "failed_samples": total_errors,
            "total_time": elapsed,
            "throughput": throughput
        }

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="The-Stack-v2 æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯")
    
    parser.add_argument("--data-dir", required=True,
                       help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir", required=True,
                       help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--service-url", required=True,
                       help="FastTextæœåŠ¡URL")
    parser.add_argument("--batch-size", type=int, default=100,
                       help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 100)")
    parser.add_argument("--max-concurrent", type=int, default=10,
                       help="æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 10)")
    parser.add_argument("--max-text-length", type=int, default=100000,
                       help="æœ€å¤§æ–‡æœ¬é•¿åº¦ (é»˜è®¤: 100000)")
    parser.add_argument("--timeout", type=int, default=120,
                       help="è¯·æ±‚è¶…æ—¶æ—¶é—´ (é»˜è®¤: 120ç§’)")
    parser.add_argument("--save-format", choices=["parquet", "json"], default="parquet",
                       help="ä¿å­˜æ ¼å¼ (é»˜è®¤: parquet)")
    parser.add_argument("--no-skip-corrupted", action="store_true",
                       help="ä¸è·³è¿‡æŸåçš„æ–‡ä»¶")
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    config = ProcessingConfig(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        service_url=args.service_url,
        batch_size=args.batch_size,
        max_concurrent=args.max_concurrent,
        max_text_length=args.max_text_length,
        timeout=args.timeout,
        skip_corrupted=not args.no_skip_corrupted,
        save_format=args.save_format
    )
    
    print("ğŸ¯ ç”Ÿäº§çº§æ•°æ®æ¸…æ´—å®¢æˆ·ç«¯")
    print("=" * 50)
    print(f"æ•°æ®ç›®å½•: {config.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"æœåŠ¡åœ°å€: {config.service_url}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"å¹¶å‘æ•°: {config.max_concurrent}")
    print(f"ä¿å­˜æ ¼å¼: {config.save_format}")
    print()
    
    try:
        async with TheStackV2Processor(config) as processor:
            result = await processor.process_all_files()
            
            # ä¿å­˜å¤„ç†æŠ¥å‘Š
            report_file = Path(config.output_dir) / "processing_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
