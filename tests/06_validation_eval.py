#!/usr/bin/env python3
"""
FastTextæœåŠ¡éªŒè¯é›†è¯„ä¼°è„šæœ¬
å¯¹æ¯”æˆ‘ä»¬çš„æœåŠ¡é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡
"""

import pandas as pd
import numpy as np
import json
import asyncio
import aiohttp
import time
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from typing import List, Dict, Any
import argparse

class FastTextValidator:
    def __init__(self, service_url: str = "http://localhost:8000"):
        self.service_url = service_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def predict_batch(self, texts: List[str], batch_size: int = 50) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹æ–‡æœ¬"""
        if not texts:
            return []
        
        # é™åˆ¶æ‰¹æ¬¡å¤§å°é¿å…è¶…æ—¶
        batch_texts = texts[:batch_size]
        
        try:
            async with self.session.post(
                f"{self.service_url}/predict",
                json=batch_texts,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„è¿”å›æ ¼å¼
                    if hasattr(self, '_first_batch_logged') is False:
                        print(f"ğŸ” æœåŠ¡è¿”å›æ ¼å¼ç¤ºä¾‹: {result[:2]}")
                        self._first_batch_logged = True
                    return result
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status}")
                    return [{"labels": ["__label__0"], "scores": [0.5]} for _ in batch_texts]
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¼‚å¸¸: {e}")
            return [{"labels": ["__label__0"], "scores": [0.5]} for _ in batch_texts]
    
    def convert_prediction(self, pred_result) -> str:
        """å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºæ ‡ç­¾"""
        # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
        if isinstance(pred_result, dict):
            labels = pred_result.get("labels", ["__label__0"])
            scores = pred_result.get("scores", [0.5])
        elif isinstance(pred_result, list) and len(pred_result) >= 2:
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ [labels, scores]
            labels = pred_result[0] if len(pred_result) > 0 else ["__label__0"]
            scores = pred_result[1] if len(pred_result) > 1 else [0.5]
        else:
            print(f"âš ï¸ æœªçŸ¥é¢„æµ‹æ ¼å¼: {pred_result}")
            return "__label__0"
        
        if not labels or not scores:
            return "__label__0"
        
        # å–å¾—åˆ†æœ€é«˜çš„æ ‡ç­¾
        max_idx = np.argmax(scores)
        raw_label = labels[max_idx]
        
        # è½¬æ¢ä¸ºFastTextæ ¼å¼
        if raw_label.startswith('__label__'):
            return raw_label
        else:
            # å¦‚æœæ˜¯ '0', '1' æ ¼å¼ï¼Œè½¬æ¢ä¸º '__label__0', '__label__1'
            return f"__label__{raw_label}"
    
    async def evaluate_validation_set(self, val_file: str, max_samples: int = None):
        """è¯„ä¼°éªŒè¯é›†"""
        print(f"ğŸ“– è¯»å–éªŒè¯é›†: {val_file}")
        
        # è¯»å–éªŒè¯é›†
        try:
            df = pd.read_parquet(val_file)
        except Exception as e:
            print(f"âŒ è¯»å–éªŒè¯é›†å¤±è´¥: {e}")
            return
        
        print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"  åˆ—å: {list(df.columns)}")
        
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['content', 'label', 'FT_label']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return
        
        # æ ·æœ¬ç»Ÿè®¡
        print(f"ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:")
        print(df['label'].value_counts())
        print(f"ğŸ“ˆ FastTextæ ‡ç­¾åˆ†å¸ƒ:")
        print(df['FT_label'].value_counts())
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ç”¨äºæµ‹è¯•
        if max_samples:
            if hasattr(self, 'random_sampling') and self.random_sampling:
                df = df.sample(n=max_samples, random_state=42)
                print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: éšæœºæŠ½æ · {len(df)} ä¸ªæ ·æœ¬")
            else:
                df = df.head(max_samples)
                print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {len(df)} ä¸ªæ ·æœ¬")
        
        # è·å–å†…å®¹å’ŒçœŸå®æ ‡ç­¾
        contents = df['content'].tolist()
        true_labels = df['FT_label'].tolist()  # ä½¿ç”¨FastTextæ ¼å¼çš„æ ‡ç­¾
        
        print(f"ğŸš€ å¼€å§‹é¢„æµ‹ {len(contents)} ä¸ªæ ·æœ¬...")
        start_time = time.time()
        
        # åˆ†æ‰¹é¢„æµ‹
        batch_size = 50
        all_predictions = []
        
        for i in range(0, len(contents), batch_size):
            batch_contents = contents[i:i+batch_size]
            batch_predictions = await self.predict_batch(batch_contents, batch_size)
            
            # è½¬æ¢é¢„æµ‹ç»“æœ
            batch_labels = [self.convert_prediction(pred) for pred in batch_predictions]
            all_predictions.extend(batch_labels)
            
            if (i // batch_size + 1) % 10 == 0:
                print(f"  å·²å¤„ç†: {i + len(batch_contents)}/{len(contents)} æ ·æœ¬")
        
        total_time = time.time() - start_time
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š ååé‡: {len(contents) / total_time:.1f} samples/sec")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        self.calculate_metrics(true_labels, all_predictions)
        
        # ä¿å­˜ç»“æœ
        result_df = df.copy()
        result_df['predicted_label'] = all_predictions
        result_df['correct'] = result_df['FT_label'] == result_df['predicted_label']
        
        output_file = f"validation_results_{int(time.time())}.parquet"
        result_df.to_parquet(output_file)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            'total_samples': len(contents),
            'predictions': all_predictions,
            'true_labels': true_labels,
            'processing_time': total_time,
            'throughput': len(contents) / total_time
        }
    
    def calculate_metrics(self, true_labels: List[str], pred_labels: List[str]):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print("=" * 80)
        
        # å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, pred_labels)
        print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            true_labels, 
            pred_labels, 
            target_names=['__label__0', '__label__1'],
            digits=4
        )
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(report)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, pred_labels)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"             é¢„æµ‹")
        print(f"çœŸå®    __label__0  __label__1")
        print(f"__label__0    {cm[0,0]:8d}    {cm[0,1]:8d}")
        print(f"__label__1    {cm[1,0]:8d}    {cm[1,1]:8d}")
        
        # ä¸åŸºå‡†å¯¹æ¯”
        print(f"\nğŸ¯ ä¸åŸºå‡†æ€§èƒ½å¯¹æ¯”:")
        print(f"åŸºå‡† __label__0: P=0.9902, R=0.8902, F1=0.9375")
        print(f"åŸºå‡† __label__1: P=0.6466, R=0.9579, F1=0.7721")
        print(f"åŸºå‡†æ•´ä½“: P=0.9023, R=0.9018, F1=0.9021")

async def main():
    parser = argparse.ArgumentParser(description='FastTextæœåŠ¡éªŒè¯é›†è¯„ä¼°')
    parser.add_argument('--val-file', 
                        default='/mnt/project/yifan/data/code/the-stack-v2_fasttext_val_with_gt.parquet',
                        help='éªŒè¯é›†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--service-url', 
                        default='http://localhost:8000',
                        help='FastTextæœåŠ¡URL')
    parser.add_argument('--max-samples', 
                        type=int,
                        help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    async with FastTextValidator(args.service_url) as validator:
        await validator.evaluate_validation_set(args.val_file, args.max_samples)

if __name__ == "__main__":
    asyncio.run(main())
