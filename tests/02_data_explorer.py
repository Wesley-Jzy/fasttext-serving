#!/usr/bin/env python3
"""
æ•°æ®æ¢ç´¢è„šæœ¬
æ·±å…¥åˆ†æthe-stack-v2æ•°æ®çš„ç»“æ„ã€å†…å®¹åˆ†å¸ƒã€æ ·æœ¬ç‰¹å¾
"""
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import numpy as np

def analyze_parquet_files(data_dir: str, max_files: int = 5) -> Dict[str, Any]:
    """åˆ†æparquetæ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯"""
    print(f"ğŸ“ åˆ†ææ•°æ®ç›®å½•: {data_dir}")
    
    if not os.path.exists(data_dir):
        print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return {}
    
    # è·å–æ‰€æœ‰parquetæ–‡ä»¶
    files = list(Path(data_dir).glob("*.parquet"))
    total_files = len(files)
    
    print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    
    if total_files == 0:
        print("âš ï¸ æœªæ‰¾åˆ°parquetæ–‡ä»¶")
        return {"total_files": 0}
    
    # åˆ†æå‰å‡ ä¸ªæ–‡ä»¶
    analyze_files = files[:max_files]
    print(f"ğŸ” è¯¦ç»†åˆ†æå‰ {len(analyze_files)} ä¸ªæ–‡ä»¶")
    
    file_info = []
    total_rows = 0
    corrupted_files = []
    
    for i, file_path in enumerate(analyze_files):
        print(f"\n--- æ–‡ä»¶ {i+1}: {file_path.name} ---")
        
        try:
            # è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            size_mb = file_path.stat().st_size / (1024*1024)
            print(f"æ–‡ä»¶å¤§å°: {size_mb:.1f}MB")
            
            # å°è¯•è¯»å–parquetæ–‡ä»¶
            df = pd.read_parquet(file_path, engine='pyarrow')
            rows = len(df)
            cols = list(df.columns)
            
            print(f"æ•°æ®è¡Œæ•°: {rows:,}")
            print(f"åˆ—å: {cols}")
            
            file_info.append({
                "filename": file_path.name,
                "size_mb": round(size_mb, 1),
                "rows": rows,
                "columns": cols
            })
            
            total_rows += rows
            
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥: {e}")
            corrupted_files.append(file_path.name)
    
    result = {
        "total_files": total_files,
        "analyzed_files": len(file_info),
        "corrupted_files": corrupted_files,
        "file_details": file_info,
        "estimated_total_rows": int(total_rows * total_files / len(analyze_files)) if file_info else 0
    }
    
    print(f"\nğŸ“ˆ æ±‡æ€»ä¿¡æ¯:")
    print(f"  æˆåŠŸåˆ†æ: {len(file_info)}/{len(analyze_files)} ä¸ªæ–‡ä»¶")
    print(f"  æŸåæ–‡ä»¶: {len(corrupted_files)} ä¸ª")
    print(f"  é¢„ä¼°æ€»è¡Œæ•°: {result['estimated_total_rows']:,}")
    
    return result

def analyze_content_column(data_dir: str, sample_size: int = 100) -> Dict[str, Any]:
    """ä¸“é—¨åˆ†æcontentåˆ—çš„ç‰¹å¾"""
    print(f"\nğŸ“ åˆ†æcontentåˆ—ç‰¹å¾ (æ ·æœ¬æ•°: {sample_size})")
    
    files = list(Path(data_dir).glob("*.parquet"))
    if not files:
        return {}
    
    content_samples = []
    
    # ä»å¤šä¸ªæ–‡ä»¶é‡‡æ ·
    for file_path in files[:3]:  # æœ€å¤šä»3ä¸ªæ–‡ä»¶é‡‡æ ·
        try:
            df = pd.read_parquet(file_path, engine='pyarrow')
            if 'content' not in df.columns:
                print(f"âš ï¸ {file_path.name} æ²¡æœ‰contentåˆ—")
                continue
            
            # éšæœºé‡‡æ ·
            sample_df = df.sample(n=min(sample_size//3, len(df)), random_state=42)
            content_samples.extend(sample_df['content'].tolist())
            
            if len(content_samples) >= sample_size:
                break
                
        except Exception as e:
            print(f"âš ï¸ é‡‡æ · {file_path.name} å¤±è´¥: {e}")
    
    if not content_samples:
        print("âŒ æ— æ³•è·å–contentæ ·æœ¬")
        return {}
    
    # åˆ†æcontentç‰¹å¾
    lengths = [len(str(content)) for content in content_samples]
    
    analysis = {
        "sample_count": len(content_samples),
        "length_stats": {
            "min": min(lengths),
            "max": max(lengths),
            "mean": np.mean(lengths),
            "median": np.median(lengths),
            "std": np.std(lengths)
        }
    }
    
    print(f"ğŸ“Š Contenté•¿åº¦ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°: {analysis['sample_count']}")
    print(f"  æœ€çŸ­: {analysis['length_stats']['min']:,} å­—ç¬¦")
    print(f"  æœ€é•¿: {analysis['length_stats']['max']:,} å­—ç¬¦")
    print(f"  å¹³å‡: {analysis['length_stats']['mean']:.0f} å­—ç¬¦")
    print(f"  ä¸­ä½æ•°: {analysis['length_stats']['median']:.0f} å­—ç¬¦")
    
    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“„ Contentæ ·æœ¬é¢„è§ˆ:")
    for i, content in enumerate(content_samples[:3]):
        content_str = str(content)
        preview = content_str[:200].replace('\n', '\\n').replace('\t', '\\t')
        print(f"  æ ·æœ¬ {i+1} (é•¿åº¦: {len(content_str)}): {preview}...")
    
    return analysis

def detect_file_patterns(data_dir: str) -> Dict[str, Any]:
    """æ£€æµ‹æ–‡ä»¶çš„å‘½åæ¨¡å¼å’Œæ—¶é—´ç‰¹å¾"""
    print(f"\nğŸ” æ£€æµ‹æ–‡ä»¶æ¨¡å¼")
    
    files = list(Path(data_dir).glob("*.parquet"))
    if not files:
        return {}
    
    # åˆ†ææ–‡ä»¶åæ¨¡å¼
    filenames = [f.name for f in files]
    
    # æŒ‰æ–‡ä»¶åæ’åº
    filenames.sort()
    
    print(f"ğŸ“‹ æ–‡ä»¶åæ¨¡å¼ (å‰10ä¸ª):")
    for name in filenames[:10]:
        print(f"  {name}")
    
    if len(filenames) > 10:
        print(f"  ... è¿˜æœ‰ {len(filenames)-10} ä¸ªæ–‡ä»¶")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æˆ³æ¨¡å¼
    patterns = {
        "total_files": len(filenames),
        "first_file": filenames[0] if filenames else "",
        "last_file": filenames[-1] if filenames else "",
        "sample_names": filenames[:5]
    }
    
    return patterns

def check_processing_feasibility(analysis_result: Dict[str, Any]) -> Dict[str, Any]:
    """è¯„ä¼°å¤„ç†å¯è¡Œæ€§"""
    print(f"\nâš–ï¸ å¤„ç†å¯è¡Œæ€§è¯„ä¼°")
    
    if not analysis_result:
        return {}
    
    total_rows = analysis_result.get("estimated_total_rows", 0)
    content_stats = analysis_result.get("content_stats", {})
    
    # ä¼°ç®—å¤„ç†æ—¶é—´å’Œèµ„æºéœ€æ±‚
    if total_rows > 0:
        # å‡è®¾å¤„ç†é€Ÿåº¦: 1000 samples/sec
        estimated_seconds = total_rows / 1000
        estimated_hours = estimated_seconds / 3600
        
        print(f"ğŸ“Š å¤„ç†ä¼°ç®—:")
        print(f"  é¢„ä¼°æ ·æœ¬æ•°: {total_rows:,}")
        print(f"  é¢„ä¼°å¤„ç†æ—¶é—´ (1K/sec): {estimated_hours:.1f} å°æ—¶")
        
        if content_stats and "length_stats" in content_stats:
            avg_length = content_stats["length_stats"]["mean"]
            max_length = content_stats["length_stats"]["max"]
            
            print(f"ğŸ“ æ–‡æœ¬ç‰¹å¾:")
            print(f"  å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
            print(f"  æœ€å¤§é•¿åº¦: {max_length:,} å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†
            if max_length > 100000:
                print("âš ï¸ å‘ç°è¶…é•¿æ–‡æœ¬ï¼Œå¯èƒ½éœ€è¦åˆ†å—å¤„ç†")
            
            if avg_length > 10000:
                print("âš ï¸ å¹³å‡æ–‡æœ¬è¾ƒé•¿ï¼Œå»ºè®®å¢åŠ å¤„ç†è¶…æ—¶æ—¶é—´")
    
    feasibility = {
        "total_samples": total_rows,
        "estimated_hours": estimated_hours if total_rows > 0 else 0,
        "needs_chunking": content_stats.get("length_stats", {}).get("max", 0) > 100000,
        "avg_text_length": content_stats.get("length_stats", {}).get("mean", 0)
    }
    
    return feasibility

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” The-Stack-v2 æ•°æ®æ¢ç´¢")
    print("=" * 50)
    
    data_dir = "/mnt/project/yifan/data/the-stack-v2-dedup_batched_download"
    
    # 1. åˆ†æparquetæ–‡ä»¶
    file_analysis = analyze_parquet_files(data_dir)
    
    # 2. åˆ†æcontentåˆ—
    content_analysis = analyze_content_column(data_dir)
    
    # 3. æ£€æµ‹æ–‡ä»¶æ¨¡å¼
    pattern_analysis = detect_file_patterns(data_dir)
    
    # 4. è¯„ä¼°å¤„ç†å¯è¡Œæ€§
    full_analysis = {
        "file_info": file_analysis,
        "content_stats": content_analysis,
        "file_patterns": pattern_analysis
    }
    
    feasibility = check_processing_feasibility(full_analysis)
    
    # ä¿å­˜åˆ†æç»“æœ
    output_file = "data_analysis_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            **full_analysis,
            "feasibility": feasibility
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ•°æ®æ¢ç´¢å®Œæˆ!")
    print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"  1. æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶")
    print(f"  2. æ‰§è¡Œæ¨¡å‹éªŒè¯: python3 tests/03_model_validator.py")

if __name__ == "__main__":
    main()
